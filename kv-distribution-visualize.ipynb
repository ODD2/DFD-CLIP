{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import statistics\n",
    "import numpy as np\n",
    "import src.clip as clip\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from accelerate import Accelerator\n",
    "from yacs.config import CfgNode as CN\n",
    "from main import get_config, init_accelerator, set_seed, FFPP\n",
    "from src.models import Detector\n",
    "logging.basicConfig(level=\"DEBUG\")\n",
    "\n",
    "\n",
    "class Obj:\n",
    "    pass\n",
    "\n",
    "\n",
    "c = FFPP.get_default_config()\n",
    "# c.augmentation = \"normal+frame\"\n",
    "# c.augmentation = \"none\"\n",
    "c.random_speed = 0\n",
    "c.pair = 1\n",
    "c.compressions = [\"c23\"]\n",
    "# c.contrast = 1\n",
    "# c.contrast_pair = 1\n",
    "c.types = [\"REAL\", \"NT\", \"DF\", \"FS\", \"F2F\"]\n",
    "# c.types = [\"NT\", \"DF\", \"FS\", \"F2F\"]\n",
    "# c.types = [\"REAL\", \"NT\"]\n",
    "# c.types = [\"REAL\"]\n",
    "\n",
    "\n",
    "mc = Detector.get_default_config()\n",
    "mc.out_dim = [2]\n",
    "mc.adapter = CN()\n",
    "mc.adapter.frozen = 0\n",
    "mc.adapter.struct = CN()\n",
    "# mc.adapter.struct.type = \"legacy-768-x-768\"\n",
    "mc.adapter.struct.type = \"768-x-768-nln\"\n",
    "mc.adapter.struct.x = 256\n",
    "mc.adapter.type = \"normal\"\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='no')\n",
    "model = Detector(mc, 20, accelerator).to(accelerator.device).eval()\n",
    "model.to(\"cuda\")\n",
    "# encoder = clip.load(\"ViT-B/16\")[0].visual.float()\n",
    "encoder = model.encoder\n",
    "# adapter = model.adapter\n",
    "# adapter.load_state_dict(\n",
    "#     {\n",
    "#         k[8:]: v for k, v in\n",
    "#         torch.load(\n",
    "#             # \"/home/od/Desktop/repos/dfd-clip/logs/comp-inv/comp-inv/mode1+256+resi+1e-2/last_weights.pt\",\n",
    "#             \"/home/od/Desktop/repos/dfd-clip/logs/test/0520T1305/best_weights.pt\",\n",
    "#             map_location=\"cpu\"\n",
    "#         ).items()\n",
    "#         if \"adapter\" in k\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "transform = model.transform\n",
    "n_px = 224\n",
    "frames = 20\n",
    "seconds = 1\n",
    "\n",
    "x = FFPP(c.clone(), frames, seconds, transform, accelerator, split=\"train\")\n",
    "_x = FFPP(\n",
    "    c.clone(),\n",
    "    frames,\n",
    "    seconds,\n",
    "    T.Compose([\n",
    "              T.Resize(n_px, interpolation=T.InterpolationMode.BICUBIC),\n",
    "              T.CenterCrop(n_px),\n",
    "              T.ConvertImageDtype(torch.float32)\n",
    "              ]),\n",
    "    accelerator,\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_idx = next(i for i, x in enumerate(x.video_list) if x[2] == \"193_030\")\n",
    "idx = next(i for i in range(len(x)) if x.video_info(i)[0] == vid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# idx = random.randint(0, len(x))\n",
    "# idx = 1825\n",
    "# idx = 1470\n",
    "# idx = 27392\n",
    "data = x[idx + 1]\n",
    "_data = _x[idx + 1]\n",
    "\n",
    "# data = [{\n",
    "#     k: data[0][i][\"c23\"]\n",
    "#     for i, k in enumerate([\"raw\", \"c23\"])\n",
    "# }]\n",
    "# _data = [{\n",
    "#     k: _data[0][i][\"c23\"]\n",
    "#     for i, k in enumerate([\"raw\", \"c23\"])\n",
    "# }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig_scale = 3\n",
    "plt.figure(figsize=(frames * fig_scale, 2 * fig_scale))\n",
    "for i, q in enumerate([\"raw\", \"c23\"]):\n",
    "    d = _data[0][q].numpy().transpose((0, 2, 3, 1))\n",
    "    for j, f in enumerate(d):\n",
    "        plt.subplot(2, frames, i * frames + j + 1)\n",
    "        plt.imshow(f)\n",
    "        plt.gca().set_yticks([i for i in range(n_px) if i % 16 == 0])\n",
    "        plt.gca().set_xticks([i for i in range(n_px) if i % 16 == 0])\n",
    "        plt.gca().xaxis.set_tick_params(labelbottom=False)\n",
    "        plt.gca().yaxis.set_tick_params(labelleft=False)\n",
    "        plt.grid(True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "# nfeatures = {}\n",
    "with torch.no_grad():\n",
    "    for k, imgs in data[0].items():\n",
    "        # get key and value from each CLIP ViT layer\n",
    "        kvs = encoder(imgs.to(\"cuda\"), with_out=True, with_q=True)\n",
    "        # discard original CLS token and restore temporal dimension\n",
    "        kvs = [{k: v[:, 1:] for k, v in kv.items()} for kv in kvs]\n",
    "        kvs = [{k: v.view(*v.shape[:2], 768).to(\"cpu\") for k, v in kv.items()} for kv in kvs]\n",
    "        features[k] = kvs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = torch.max(\n",
    "#     torch.abs(\n",
    "#         (features[\"c23\"][0][\"k\"] - torch.mean(features[\"c23\"][0][\"k\"], dim=0)) /\n",
    "#         torch.var(features[\"c23\"][0][\"k\"], dim=0)\n",
    "#     ).mean(dim=-1),\n",
    "#     dim=0\n",
    "# )[0]\n",
    "# features[\"raw\"][0][\"k\"].shape, features[\"raw\"][0][\"k\"][0, 14 * 4 + 4].shape\n",
    "# torch.nn.functional.cosine_similarity(\n",
    "#     features[\"raw\"][0][\"k\"],\n",
    "#     features[\"raw\"][0][\"k\"][0, 14 * 4 + 4],\n",
    "#     dim=-1\n",
    "# ).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This scenario shows the distribution of each subject(k,v,out)\n",
    "# # throughout the temporal dimension of each ViT layer\n",
    "# methods = {}\n",
    "# for m in [\"raw\", \"c23\"]:\n",
    "#     l = {\n",
    "#         \"k\": [],\n",
    "#         \"v\": [],\n",
    "#         \"out\": []\n",
    "#     }\n",
    "#     for l2 in range(12):\n",
    "#         for subject in [\"k\", \"v\", \"out\"]:\n",
    "#             l[subject].append(\n",
    "#                 torch.var(features[m][l2][subject], dim=0).mean(dim=-1).view((14, 14))\n",
    "#                 # torch.max(\n",
    "#                 #     torch.abs(\n",
    "#                 #         (features[m][l2][subject] - torch.mean(features[m][l2][subject], dim=0)) /\n",
    "#                 #         torch.sqrt(torch.var(features[m][l2][subject], dim=0))\n",
    "#                 #     ).mean(dim=-1),\n",
    "#                 #     dim=0\n",
    "#                 # )[0].view(14, 14)\n",
    "#             )\n",
    "#     methods[m] = l\n",
    "\n",
    "\n",
    "# for name, l in methods.items():\n",
    "#     plt.figure(figsize=(36, 9), layout=\"constrained\")\n",
    "#     plt.suptitle(f'{name}', fontsize=12)\n",
    "#     for j, s in enumerate([\"k\", \"v\", \"out\"]):\n",
    "#         for i, v in enumerate(l[s]):\n",
    "#             plt.subplot(3, 12, j * 12 + i + 1)\n",
    "#             im = plt.imshow(v)\n",
    "\n",
    "#             # Show all ticks and label them with the respective list entries\n",
    "#             plt.gca().set_xticks(np.arange(14))\n",
    "#             plt.gca().set_yticks(np.arange(14))\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scenario shows the similarity given a patch location of the first frame and all video patches.\n",
    "# The result is an temporally averaged value for each patch location.\n",
    "methods = {}\n",
    "subjects = ['q', 'k', 'v', 'out']\n",
    "# qualities = [\"raw\", \"c23\"]\n",
    "# subjects = ['k']\n",
    "qualities = [\"c23\"]\n",
    "patch_loc = 14 * 1 + 7\n",
    "for m in qualities:\n",
    "    l = {\n",
    "        s: []\n",
    "        for s in subjects\n",
    "    }\n",
    "    for l2 in range(12):\n",
    "        for subject in subjects:\n",
    "            l[subject].append(\n",
    "                (torch.nn.functional.cosine_similarity(\n",
    "                    features[m][l2][subject],\n",
    "                    features[m][l2][subject][0, patch_loc],\n",
    "                    dim=-1\n",
    "                ) / (768 ** 0.5)).softmax(dim=-1).view((-1, 14, 14)).permute(1, 0, 2).flatten(1, 2)\n",
    "            )\n",
    "    methods[m] = l\n",
    "\n",
    "# This section calculates the difference of the raw and c23 attention scores.\n",
    "# methods[\"diff\"] = {}\n",
    "# for s in subjects:\n",
    "#     methods[\"diff\"][s] = []\n",
    "#     for l2 in range(12):\n",
    "#         methods[\"diff\"][s].append(torch.abs(methods[\"raw\"][s][l2] - methods[\"c23\"][s][l2]))\n",
    "\n",
    "for name, l in methods.items():\n",
    "    for _, s in enumerate(subjects):\n",
    "        plt.figure(figsize=(60, 36), layout=\"constrained\")\n",
    "        plt.suptitle(f'{name}-{s}', fontsize=12)\n",
    "        for j in range(12):\n",
    "            plt.subplot(12, 1, j + 1)\n",
    "\n",
    "            im = plt.imshow(l[s][j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a multi-sample version of the methods above which compares the specified patch location in the first frame\n",
    "# and calculates the attentional score with the remaining patches throughout the video clip.\n",
    "\n",
    "# import random\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# subjects = ['q', 'k', 'v', 'out']\n",
    "# # qualities = [\"raw\", \"c23\"]\n",
    "# # subjects = ['k']\n",
    "# qualities = [\"c23\"]\n",
    "\n",
    "\n",
    "# methods = {\n",
    "#     q: {\n",
    "#         s: [torch.zeros((14, frames * 14)) for _ in range(12)]\n",
    "#         for s in subjects\n",
    "#     } for q in qualities\n",
    "# }\n",
    "\n",
    "# for i in tqdm(range(20)):\n",
    "#     ########## random select index ############\n",
    "#     idx = random.randint(0, len(x))\n",
    "#     data = x[idx]\n",
    "\n",
    "#     ######### extract video features ###########\n",
    "#     features = {}\n",
    "#     with torch.no_grad():\n",
    "#         for k, imgs in data[0].items():\n",
    "#             # get key and value from each CLIP ViT layer\n",
    "#             kvs = encoder(imgs.to(\"cuda\"), with_out=True, with_q=True)\n",
    "#             # discard original CLS token and restore temporal dimension\n",
    "#             kvs = [{k: v[:, 1:] for k, v in kv.items()} for kv in kvs]\n",
    "#             kvs = [{k: v.view(*v.shape[:2], 768).to(\"cpu\") for k, v in kv.items()} for kv in kvs]\n",
    "#             features[k] = kvs\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     ######### extract video features ###########\n",
    "#     for m in qualities:\n",
    "#         for l2 in range(12):\n",
    "#             for subject in subjects:nose\n",
    "#                 methods[m][subject][l2] += (torch.nn.functional.cosine_similarity(\n",
    "#                     features[m][l2][subject],\n",
    "#                     features[m][l2][subject][0, 14 * 2 + 5],\n",
    "#                     dim=-1\n",
    "#                 ) / (768**0.5)).softmax(dim=-1).view((-1, 14, 14)).permute(1, 0, 2).flatten(1, 2) / 100\n",
    "\n",
    "\n",
    "# for name, l in methods.items():\n",
    "#     for _, s in enumerate(subjects):\n",
    "#         plt.figure(figsize=(60, 36), layout=\"constrained\")\n",
    "#         plt.suptitle(f'{name}-{s}', fontsize=12)\n",
    "#         for j in range(12):\n",
    "#             plt.subplot(12, 1, j + 1)\n",
    "\n",
    "#             im = plt.imshow(l[s][j])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section collects patches of specific locations and calculates the mean of patches as the desire semantic meaning.\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "semantic_locations = {\n",
    "    \"eyes\": [\n",
    "        [4, 3], [4, 4], [4, 9], [4, 10]\n",
    "    ],\n",
    "    \"nose\": [\n",
    "        [7, 6], [6, 6], [5, 6]\n",
    "    ],\n",
    "    \"lips\": [\n",
    "        [10, 5], [10, 6], [10, 7]\n",
    "    ],\n",
    "    \"eyebrows\": [\n",
    "        [2, 3], [2, 4], [3, 4], [3, 5], [3, 8], [3, 9], [2, 9], [2, 10]\n",
    "    ],\n",
    "    \"skin\": [\n",
    "        [0, 6], [0, 7], [1, 6], [1, 7],\n",
    "        [7, 3], [7, 4], [7, 10], [7, 11],\n",
    "        [11, 6], [11, 7], [12, 6], [12, 7]\n",
    "    ]\n",
    "}\n",
    "\n",
    "semantic_locations = {k: [_v[0] * 14 + _v[1] for _v in v] for k, v in semantic_locations.items()}\n",
    "\n",
    "subjects = ['q', 'k', 'v', 'out']\n",
    "# qualities = [\"raw\", \"c23\"]\n",
    "# subjects = ['k']\n",
    "qualities = [\"c23\"]\n",
    "\n",
    "semantic_patches = {\n",
    "    s: {\n",
    "        k: [[] for _ in range(12)]\n",
    "        for k in semantic_locations.keys()\n",
    "    }\n",
    "    for s in subjects\n",
    "}\n",
    "\n",
    "methods = {\n",
    "    q: {\n",
    "        s: [torch.zeros((14, frames * 14)) for _ in range(12)]\n",
    "        for s in subjects\n",
    "    } for q in qualities\n",
    "}\n",
    "\n",
    "for i in range(100):\n",
    "    ########## random select index ############\n",
    "    idx = random.randint(0, len(x))\n",
    "    data = x[idx]\n",
    "\n",
    "    ######### extract video features ###########\n",
    "    features = {}\n",
    "    with torch.no_grad():\n",
    "        for k, imgs in data[0].items():\n",
    "            # get key and value from each CLIP ViT layer\n",
    "            kvs = encoder(imgs[0].unsqueeze(0).to(\"cuda\"), with_out=True, with_q=True)\n",
    "            # discard original CLS token and restore temporal dimension\n",
    "            kvs = [{k: v[:, 1:] for k, v in kv.items()} for kv in kvs]\n",
    "            kvs = [{k: v.view(*v.shape[:2], 768).to(\"cpu\") for k, v in kv.items()} for kv in kvs]\n",
    "            features[k] = kvs\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ######### extract video features ###########\n",
    "    for m in qualities:\n",
    "        for l2 in range(12):\n",
    "            for subject in subjects:\n",
    "                for part, indices in semantic_locations.items():\n",
    "                    semantic_patches[subject][part][l2].extend(features[m][l2][subject][0, indices].tolist())\n",
    "\n",
    "semantic_patches = {\n",
    "    s: {\n",
    "        k: [\n",
    "            torch.tensor(semantic_patches[s][k][i]).mean(dim=0)\n",
    "            for i in range(12)\n",
    "        ]\n",
    "        for k in semantic_locations.keys()\n",
    "    }\n",
    "    for s in subjects\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section shows the similarity score given a semantic patch embedding from the collected patches of a previous phase.\n",
    "methods = {}\n",
    "# subjects = ['q', 'k', 'v', 'out']\n",
    "subjects = ['k']\n",
    "# qualities = [\"raw\", \"c23\"]\n",
    "# subjects = ['k']\n",
    "qualities = [\"c23\"]\n",
    "patch_loc = 14 * 11 + 8\n",
    "for m in qualities:\n",
    "    l = {\n",
    "        s: []\n",
    "        for s in subjects\n",
    "    }\n",
    "    for l2 in range(12):\n",
    "        for subject in subjects:\n",
    "            l[subject].append(\n",
    "                (torch.nn.functional.cosine_similarity(\n",
    "                    features[m][l2][subject],\n",
    "                    semantic_patches[\"q\"][\"eyes\"][l2],\n",
    "                    dim=-1\n",
    "                ) / (768 ** 0.5)).softmax(dim=-1).view((-1, 14, 14)).permute(1, 0, 2).flatten(1, 2)\n",
    "            )\n",
    "    methods[m] = l\n",
    "\n",
    "\n",
    "for name, l in methods.items():\n",
    "    for _, s in enumerate(subjects):\n",
    "        plt.figure(figsize=(60, 36), layout=\"constrained\")\n",
    "        plt.suptitle(f'{name}-{s}', fontsize=12)\n",
    "        for j in range(12):\n",
    "            plt.subplot(12, 1, j + 1)\n",
    "\n",
    "            im = plt.imshow(l[s][j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"misc/semantic_patches.pickle\", \"wb\") as f:\n",
    "    pickle.dump(semantic_patches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9d400f0712968c6b0c2c185442708fcff6ca365f2120d94e1d89a9df5bd30ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
