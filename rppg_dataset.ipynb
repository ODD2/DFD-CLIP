{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pyedflib import highlevel as reader\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from src.datasets import SessionMeta,RPPG\n",
    "import torch\n",
    "import pickle\n",
    "# read an edf file\n",
    "# signals, signal_headers, header = reader.read_edf('/storage/HCI/Sessions/59/Part_1_Trial18_taggingImages1.bdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_labels = [x[\"label\"] for x in signal_headers]\n",
    "# print(signal_labels,f\"len:{len(signal_labels)}\")\n",
    "# print(\"=============================\")\n",
    "# print(signal_labels.index(\"EXG1\"))\n",
    "# print(signal_labels.index(\"EXG2\"))\n",
    "# print(signal_labels.index(\"EXG3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hr_freq = signal_headers[32][\"sample_frequency\"]\n",
    "# print(hr_freq)\n",
    "# hr_skip = int(hr_freq * 30)\n",
    "# hr_dur = int(hr_freq * 3)\n",
    "# hr_data = signals[33][hr_skip:hr_skip + hr_dur]\n",
    "# plt.plot([i/hr_freq for i in range(len(hr_data))],hr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_freq = signal_headers[46][\"sample_frequency\"]\n",
    "# event_skip = int(event_freq * 29)\n",
    "# event_data = signals[46][event_skip:-event_skip]\n",
    "# plt.plot([i/event_freq for i in range(len(event_data))],event_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamps = []\n",
    "# prev = None\n",
    "# for i,v in enumerate(event_data):\n",
    "#     if(not v == prev):\n",
    "#         timestamps.append(i)\n",
    "#         prev = v\n",
    "# timestamps\n",
    "# print(f\"stimuate duration: {(timestamps[-2] - timestamps[1])/event_freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = SessionMeta(\"./datasets/hci/Sessions/521/\")\n",
    "# test.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(level=\"DEBUG\")\n",
    "# test.require_check(\n",
    "#     video=True,\n",
    "#     bdf=True,\n",
    "#     time=True,\n",
    "#     alt_video_folder=os.path.join(\"cropped_faces\",\"c23\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = RPPG.get_default_config()\n",
    "c.clip_duration = 10\n",
    "c.num_frames = c.clip_duration * 5\n",
    "c.train_ratio = 1.0\n",
    "# c.meta_folder=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.models import Detector\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from src.models import Detector\n",
    "# from main import get_config,init_accelerator,set_seed\n",
    "# from torchinfo import summary\n",
    "\n",
    "# config = get_config(\"./configs/rppg.yml\")\n",
    "\n",
    "# # initialize accelerator and trackers (if enabled)\n",
    "# accelerator = init_accelerator(config)\n",
    "# model = Detector(config.model, 50, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, scandir, makedirs\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import VideoReader\n",
    "from tqdm.auto import tqdm\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import heartpy as hp\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "from scipy.signal import resample\n",
    "from pyedflib import highlevel as reader\n",
    "\n",
    "import logging\n",
    "class RPPG(Dataset):\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        C.name = 'train'\n",
    "        C.root_dir = './datasets/hci/'\n",
    "        C.detection_level = 'video'\n",
    "        C.train_ratio = 0.95\n",
    "        C.scale = 1.0\n",
    "        C.cropped_folder=\"cropped_faces\"\n",
    "        C.meta_folder=\"Metas\"\n",
    "        C.dataset=\"RPPG\"\n",
    "        C.compressions = [\"raw\"]\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config,num_frames,clip_duration, transform=None, accelerator=None, split='train',index=0):\n",
    "        assert 0 <= config.scale <= 1\n",
    "        assert 0 <= config.train_ratio <= 1\n",
    "        assert split in [\"train\",\"val\"]\n",
    "\n",
    "        # TODO: accelerator not implemented\n",
    "        self.name = config.name\n",
    "        # HCI datasets recorded videos with 61 fps\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.clip_duration = clip_duration\n",
    "        self.index = index\n",
    "        self.scale = config.scale\n",
    "        self.compressions = config.compressions\n",
    "        self.cropped_folder =  config.cropped_folder\n",
    "        \n",
    "        # dataset consistency\n",
    "        rng = random.Random()\n",
    "        rng.seed(777)\n",
    "        session_dirs = sorted(glob(path.join(config.root_dir,\"Sessions\",\"*\")))\n",
    "        rng.shuffle(session_dirs)\n",
    "\n",
    "        # dataset splitting\n",
    "        if split == \"train\":\n",
    "            target_sessions = session_dirs[:int(len(session_dirs)*config.train_ratio*self.scale)]\n",
    "        elif split == \"val\":\n",
    "            target_sessions = session_dirs[int(len(session_dirs)*((1-config.train_ratio)*(1-self.scale) + config.train_ratio)):]\n",
    "        \n",
    "        # speed up dataset initialization\n",
    "        if (not config.meta_folder):\n",
    "            logging.info(\"Meta folder unspecified, building session meta infos....\")\n",
    "            self.session_metas = [\n",
    "                SessionMeta(\n",
    "                    session_dir\n",
    "                ) \n",
    "                for session_dir in target_sessions\n",
    "            ]\n",
    "        else:\n",
    "            logging.info(\"Meta folder specified, loading meta infos....\")\n",
    "            self.session_metas = [None for _ in range(len(target_sessions))]\n",
    "            for i,session_dir in tqdm(enumerate(target_sessions)):\n",
    "                try:\n",
    "                    with open(path.join(session_dir.replace(\"Sessions\",config.meta_folder),\"meta.pickle\"),\"rb\") as file:\n",
    "                        self.session_metas[i] = pickle.load(file)\n",
    "                except Exception as e:\n",
    "                    logging.debug(f\"Error while loading meta pickle: '{e}'\")\n",
    "\n",
    "        logging.info(\"Session meta created.\")\n",
    "        logging.debug(f\"Number of session metas before checking: {len(self.session_metas)}\")\n",
    "        # remove erroneous/missing datas.\n",
    "        for comp in self.compressions:\n",
    "            self.session_metas = [\n",
    "                meta for meta in self.session_metas \n",
    "                if meta and meta.require_check(\n",
    "                    video=True,\n",
    "                    bdf=True,\n",
    "                    time=True,\n",
    "                    video_folders=[path.join(config.cropped_folder,comp)]\n",
    "                )\n",
    "            ]\n",
    "        logging.debug(f\"Number of session metas after checking: {len(self.session_metas)}\")\n",
    "\n",
    "        # calculate available clips per session\n",
    "        self.session_clips = [int(meta.duration // self.clip_duration) for meta in self.session_metas]\n",
    "\n",
    "        # stacking up the amount of session clips for further usage\n",
    "        self.stack_session_clips = [0]\n",
    "        for i in self.session_clips:\n",
    "            self.stack_session_clips.append(self.stack_session_clips[-1] + i)\n",
    "        self.stack_session_clips.pop(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.stack_session_clips[-1]*len(self.compressions)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        result = self.get_dict(idx)\n",
    "        return result[\"frames\"],result[\"label\"],result[\"mask\"],self.index\n",
    "        \n",
    "    def get_dict(self,idx):\n",
    "        while(True):\n",
    "            try:\n",
    "                comp = self.compressions[int(idx //self.stack_session_clips[-1])]\n",
    "                idx = idx % self.stack_session_clips[-1]\n",
    "                session_idx =  next(i for i,x in enumerate(self.stack_session_clips) if  idx < x)\n",
    "                session_meta = self.session_metas[session_idx]\n",
    "                session_offset_duration =  (idx - (0 if session_idx == 0 else self.stack_session_clips[session_idx-1]))*self.clip_duration\n",
    "                # heart rate data processing\n",
    "                signals, signal_headers, _ = reader.read_edf(session_meta.bdf_path,ch_names=[\"EXG1\",\"EXG2\",\"EXG3\",\"Status\"])\n",
    "                _hr_datas = []\n",
    "                for hr_channel_idx in range(3):\n",
    "                    try:\n",
    "                        assert  int(session_meta.session_hr_sample_freq) == int(signal_headers[hr_channel_idx][\"sample_frequency\"])\n",
    "                        # - the ERG sample frequency\n",
    "                        hr_sample_freq =  session_meta.session_hr_sample_freq\n",
    "                        # - the amount of samples to skip, including the 30s stimulation offset and session clip offset.\n",
    "                        hr_sample_offset =  session_meta.flag_hr_beg_sample + int(session_offset_duration*hr_sample_freq)\n",
    "                        # - the amount of samples for the duration of a clip\n",
    "                        hr_clip_samples = int(hr_sample_freq*self.clip_duration)\n",
    "                        # - fetch heart rate data of clip duration\n",
    "                        _hr_data = signals[hr_channel_idx][hr_sample_offset:hr_sample_offset + hr_clip_samples]\n",
    "                        # - preprocess the ERG data: filter out the noise.\n",
    "                        _hr_data = hp.filter_signal(_hr_data, cutoff = 0.05, sample_rate = session_meta.session_hr_sample_freq, filtertype='notch')\n",
    "                        # - scale down the ERG value to 3.4 max.\n",
    "                        _hr_data = (_hr_data - _hr_data.min())/(_hr_data.max()-_hr_data.min()) * 3.4\n",
    "                        # - resample the ERG\n",
    "                        _hr_data = resample(_hr_data, len(_hr_data) * 4)\n",
    "                        # - process the ERG data: get measurements.\n",
    "                        _wd, _measures = hp.process(hp.scale_data(_hr_data),session_meta.session_hr_sample_freq * 4)\n",
    "                        # - nan/error check\n",
    "                        if(_measures[\"bpm\"] > 180 or _measures[\"bpm\"] < 41):\n",
    "                            continue\n",
    "\n",
    "                        for v in _measures.values():\n",
    "                            # ignore\n",
    "                            if type(v)==float and math.isnan(v):\n",
    "                                break\n",
    "                        else:\n",
    "                            # - save for comparison.\n",
    "                            _hr_datas.append((_hr_data,_measures,_wd))\n",
    "                    except Exception as e:\n",
    "                        logging.debug(f\"Error occur during heart rate analysis for index {idx}:{e}\")\n",
    "                        continue\n",
    "                \n",
    "                if(len(_hr_datas) == 0):\n",
    "                    raise Exception(f\"Unable to process the ERG data for index {idx}\")\n",
    "                \n",
    "                # get the best ERG measurement result with the sdnn\n",
    "                best_pair = sorted(_hr_datas,key=lambda x : x[1][\"sdnn\"])[0]\n",
    "                hr_data,measures,wd = best_pair[0], best_pair[1], best_pair[2]\n",
    "                bpm = measures[\"bpm\"]\n",
    "                label = torch.tensor([1/(pow(2*math.pi,0.5))*pow(math.e,(-pow((k-(bpm-41)),2)/2)) for k in range(180)])\n",
    "\n",
    "                # video frame processing\n",
    "                frames = []\n",
    "                comp_video_path = session_meta.video_path.replace(\n",
    "                    \"Sessions\",\n",
    "                    path.join(\"Sessions\" if not self.cropped_folder else self.cropped_folder,comp)\n",
    "                )\n",
    "                logging.debug(f\"loading video: {comp_video_path}\")\n",
    "                cap = cv2.VideoCapture(comp_video_path)\n",
    "                assert int(session_meta.session_video_sample_freq) == int(cap.get(cv2.CAP_PROP_FPS)), f\"{int(session_meta.session_video_sample_freq)},{int(cap.get(cv2.CAP_PROP_FPS))}\"\n",
    "                video_sample_freq = session_meta.session_video_sample_freq\n",
    "                # - the amount of frames to skip\n",
    "                video_sample_offset = int(session_meta.flag_video_beg_sample - session_meta.session_video_beg_sample) + int(video_sample_freq * session_offset_duration)\n",
    "                # - the amount of frames for the duration of a clip\n",
    "                video_clip_samples = int(session_meta.session_video_sample_freq * self.clip_duration)\n",
    "                # - the amount of frames to skip in order to meet the num_frames per clip.(excluding the head & tail frames )\n",
    "                video_sample_stride = (video_clip_samples-1) / (self.num_frames - 1)\n",
    "                # - fast forward to the the sampling start.\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES,video_sample_offset)\n",
    "                # - fetch frames of clip duration\n",
    "                next_sample_idx = 0\n",
    "                for sample_idx in range(video_clip_samples):\n",
    "                    ret, frame = cap.read()\n",
    "                    if(ret):\n",
    "                        if(sample_idx == next_sample_idx):\n",
    "                            frames.append(torch.from_numpy(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB).transpose((2,0,1))))\n",
    "                            next_sample_idx = int(round(len(frames) * video_sample_stride))\n",
    "                    else:\n",
    "                        raise NotImplementedError()\n",
    "                frames = torch.stack(frames)\n",
    "\n",
    "                # transformation\n",
    "                if (self.transform):\n",
    "                    frames = self.transform(frames)\n",
    "\n",
    "                # padding and masking missing frames.\n",
    "                mask = torch.tensor([1.] * len(frames) +\n",
    "                                    [0.] * (self.num_frames - len(frames)), dtype=torch.bool)\n",
    "                if len(frames) < self.num_frames:\n",
    "                    diff = self.num_frames - len(frames)\n",
    "                    padding = torch.zeros((diff, *frames.shape[1:]),dtype=torch.uint8)\n",
    "                    frames = torch.concatenate((frames, padding))\n",
    "                \n",
    "                return {\n",
    "                    \"frames\":frames,\n",
    "                    \"label\":label,\n",
    "                    \"mask\":mask,\n",
    "                    \"hr_data\":hr_data,\n",
    "                    \"measures\":measures,\n",
    "                    \"wd\":wd\n",
    "                }\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error occur: {e}\")\n",
    "                idx = random.randrange(0,len(self))\n",
    "\n",
    "    def save_meta(self,meta_folder=\"Metas\"):\n",
    "        for meta in self.session_metas:\n",
    "            meta_dir = meta.session_dir.replace(\"Sessions\",meta_folder)\n",
    "            makedirs(meta_dir,exist_ok=True)\n",
    "            with open(path.join(meta_dir,\"meta.pickle\"),\"wb\") as file:\n",
    "                pickle.dump(obj=meta,file=file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.meta_folder = None\n",
    "c.compressions = [\"c23\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=\"DEBUG\")\n",
    "x = RPPG(c,50,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.session_metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = x.get_dict(1000)\n",
    "frames,label,hr_data, masks, measures,wd = r[\"frames\"],r[\"label\"],r[\"hr_data\"],r[\"mask\"],r[\"measures\"],r[\"wd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30,150))\n",
    "plt.imshow(np.stack(frames.numpy().transpose((0,2,3,1)),axis=1).reshape((150,-1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heartpy as hp\n",
    "hp.plotter(wd, measures)\n",
    "#display computed measures\n",
    "for measure in measures.keys():\n",
    "    print('%s: %f' %(measure, measures[measure]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(x))):\n",
    "    try:\n",
    "        x[i]\n",
    "    except Exception as e:\n",
    "        print(i,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from src.datasets import resample\n",
    "self = x\n",
    "idx = 199\n",
    "\n",
    "session_idx =  next(i for i,x in enumerate(self.stack_session_clips) if  idx < x)\n",
    "session_meta = self.session_metas[session_idx]\n",
    "session_offset_duration =  (idx - (0 if session_idx == 0 else self.stack_session_clips[session_idx-1]))*self.clip_duration\n",
    "# heart rate data processing\n",
    "signals, signal_headers, _ = reader.read_edf(session_meta.bdf_path,ch_names=[\"EXG1\",\"EXG2\",\"EXG3\",\"Status\"])\n",
    "_hr_datas = []\n",
    "for hr_channel_idx in range(3):\n",
    "    try:\n",
    "        assert  int(session_meta.session_hr_sample_freq) == int(signal_headers[hr_channel_idx][\"sample_frequency\"])\n",
    "        # - the ERG sample frequency\n",
    "        hr_sample_freq =  session_meta.session_hr_sample_freq\n",
    "        # - the amount of samples to skip, including the 30s stimulation offset and session clip offset.\n",
    "        hr_sample_offset =  session_meta.flag_hr_beg_sample + int(session_offset_duration*hr_sample_freq)\n",
    "        # - the amount of samples for the duration of a clip\n",
    "        hr_clip_samples = int(hr_sample_freq*self.clip_duration)\n",
    "        # - fetch heart rate data of clip duration\n",
    "        _hr_data = signals[hr_channel_idx][hr_sample_offset:hr_sample_offset + hr_clip_samples]\n",
    "        # - preprocess the ERG data: filter out the noise.\n",
    "        _hr_data = hp.filter_signal(_hr_data, cutoff = 0.05, sample_rate = session_meta.session_hr_sample_freq, filtertype='notch')\n",
    "        # - scale down the ERG value to 3.4 max.\n",
    "        _hr_data = (_hr_data - _hr_data.min())/(_hr_data.max()-_hr_data.min()) * 3.4\n",
    "        # - resample the ERG\n",
    "        _hr_data = resample(_hr_data, len(_hr_data) * 10)\n",
    "        # - process the ERG data: get measurements.\n",
    "        _wd, _measures = hp.process(hp.scale_data(_hr_data),session_meta.session_hr_sample_freq * 10)\n",
    "        # - nan/error check\n",
    "        # if(_measures[\"bpm\"] > 180 or _measures[\"bpm\"] < 41):\n",
    "            # continue\n",
    "    \n",
    "        for v in _measures.values():\n",
    "            # ignore\n",
    "            if type(v)==float and math.isnan(v):\n",
    "                break\n",
    "        else:\n",
    "            # - save for comparison.\n",
    "            _hr_datas.append((_hr_data,_measures,_wd))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occur during heart rate analysis for index {idx}:{e}\")\n",
    "        continue\n",
    "\n",
    "if(len(_hr_datas) == 0):\n",
    "    raise Exception(f\"Unable to process the ERG data for index {idx}\")\n",
    "\n",
    "# get the best ERG measurement result with the sdnn\n",
    "best_pair = sorted(_hr_datas,key=lambda x : x[1][\"sdnn\"])[0]\n",
    "hr_data,measures,wd = best_pair[0], best_pair[1], best_pair[2]\n",
    "bpm = measures[\"bpm\"]\n",
    "1/(pow(2*math.pi,0.5))*pow(math.e,(-pow((31-(72-41)),2)/2))\n",
    "label = torch.tensor([1/(pow(2*math.pi,0.5))*pow(math.e,(-pow((k-(bpm-41)),2)/2)) for k in range(180)])\n",
    "\n",
    "import heartpy as hp\n",
    "hp.plotter(wd, measures)\n",
    "#display computed measures\n",
    "for measure in measures.keys():\n",
    "    print('%s: %f' %(measure, measures[measure]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9d400f0712968c6b0c2c185442708fcff6ca365f2120d94e1d89a9df5bd30ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
