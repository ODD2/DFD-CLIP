{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import torch\n",
                "import random\n",
                "import pickle\n",
                "import logging\n",
                "import statistics\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "import src.clip as CLIP\n",
                "from torchinfo import summary\n",
                "import matplotlib.pyplot as plt\n",
                "import torchvision.transforms as T\n",
                "import albumentations as alb\n",
                "from accelerate import Accelerator\n",
                "from yacs.config import CfgNode as CN\n",
                "from main import get_config, init_accelerator, set_seed, FFPP\n",
                "from src.models import Detector\n",
                "# logging.basicConfig(level=\"DEBUG\")\n",
                "\n",
                "\n",
                "class Obj:\n",
                "    pass\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def extract_features(encoder, clip):\n",
                "    # get attributes from each CLIP ViT layer\n",
                "    kvs = encoder(clip.to(\"cuda\"), with_out=True, with_q=True)\n",
                "    # discard original CLS token and restore temporal dimension\n",
                "    for i in range(len(kvs)):\n",
                "        for k in kvs[i].keys():\n",
                "            kvs[i][k] = kvs[i][k][:, 1:].to(\"cpu\")\n",
                "            if (not k == \"out\"):\n",
                "                kvs[i][k] = kvs[i][k].flatten(-2)\n",
                "    torch.cuda.empty_cache()\n",
                "    return kvs\n",
                "\n",
                "\n",
                "def var(x, patch_num, *args, **kargs):\n",
                "    # shows the variance of the video patches over the temporal dimension.\n",
                "    return (\n",
                "        torch.var(x, dim=0)\n",
                "        .mean(dim=-1)\n",
                "        .view((patch_num, patch_num))\n",
                "        .unsqueeze(-1)\n",
                "    )\n",
                "\n",
                "\n",
                "def max_stdev(x, patch_num, *args, **kargs):\n",
                "    # shows the maximum stdev value of the video patches averaged over the temporal dimension.\n",
                "    return (\n",
                "        torch.max(\n",
                "            torch.abs((x - torch.mean(x, dim=0)) / torch.sqrt(torch.var(x, dim=0))).mean(dim=-1),\n",
                "            dim=0\n",
                "        )[0]\n",
                "        .view(patch_num, patch_num)\n",
                "        .unsqueeze(-1)\n",
                "    )\n",
                "\n",
                "\n",
                "def one_patch_cos_sim(x, t, c, patch_num, *args, **kargs):\n",
                "    # shows the video patch similarities given a patch location\n",
                "    return (\n",
                "        torch.nn.functional.cosine_similarity(x, x[t, c], dim=-1)\n",
                "        .view((-1, patch_num, patch_num))\n",
                "        .permute(1, 0, 2)\n",
                "        .flatten(1, 2)\n",
                "        .unsqueeze(-1)\n",
                "    )\n",
                "\n",
                "\n",
                "def semantic_patch_cos_sim(x, patch_num, part, _s, _l, semantic_patches, s=None, *args, **kargs):\n",
                "    # shows the video patch similarities given a semantic patch\n",
                "    # _l -> the layer of the feature x\n",
                "    # s -> the mandatory subject, overwrites _s\n",
                "    # _s -> the subject of the feature x\n",
                "    return (\n",
                "        torch.nn.functional.cosine_similarity(\n",
                "            x,\n",
                "            semantic_patches[_s if s == None else s][part][_l],\n",
                "            dim=-1\n",
                "        )\n",
                "        .view((-1, patch_num, patch_num))\n",
                "        .permute(1, 0, 2)\n",
                "        .flatten(1, 2)\n",
                "        .unsqueeze(-1)\n",
                "    )\n",
                "\n",
                "\n",
                "def plotter(features, title=\"\", mode=\"subject-layer\", num_layers=16, unit_size=3, font_size=12):\n",
                "    keys = list(features.keys())\n",
                "    num_keys = len(keys)\n",
                "    num_layers = len(features[keys[0]])\n",
                "    num_frames = features[keys[0]][0].shape[0]\n",
                "\n",
                "    def create():\n",
                "        if mode == \"subject-layer\":\n",
                "            plt.figure(figsize=(unit_size * num_layers, unit_size * num_keys), layout=\"constrained\")\n",
                "            plt.suptitle(title, fontsize=font_size)\n",
                "        elif mode == \"layer-frame\":\n",
                "            plt.figure(figsize=(unit_size * num_frames, unit_size * num_layers), layout=\"constrained\")\n",
                "            plt.suptitle(title, fontsize=font_size)\n",
                "\n",
                "    def show():\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "\n",
                "    if mode == \"subject-layer\":\n",
                "        create()\n",
                "        for j, s in enumerate(features.keys()):\n",
                "            for i, v in enumerate(features[s]):\n",
                "                plt.subplot(num_keys, num_layers, j * num_layers + i + 1)\n",
                "                plt.title(f\"L{i}-{s.upper()}\")\n",
                "                plt.gca().axis(\"off\")\n",
                "                plt.imshow(v)\n",
                "        show()\n",
                "\n",
                "    elif mode == \"layer-frame\":\n",
                "        for j, s in enumerate(features.keys()):\n",
                "            create()\n",
                "            for i, v in enumerate(features[s]):\n",
                "                plt.subplot(num_layers, 1, i + 1)\n",
                "                plt.title(f\"L{i}-{s.upper()}\")\n",
                "                plt.gca().axis(\"off\")\n",
                "                plt.imshow(v)\n",
                "            show()\n",
                "    else:\n",
                "        raise NotImplementedError()\n",
                "\n",
                "\n",
                "def driver(features, method, subjects=None, patch_num=14, **kargs):\n",
                "    if subjects == None:\n",
                "        subjects = list(features[0].keys())\n",
                "\n",
                "    assert features[0][subjects[0]].shape[1] == patch_num**2\n",
                "\n",
                "    r = {\n",
                "        k: [] for k in subjects\n",
                "    }\n",
                "    num_layers = len(features)\n",
                "    for l in range(num_layers):\n",
                "        for s in subjects:\n",
                "            # variance\n",
                "            r[s].append(method(features[l][s], patch_num=patch_num, _l=l, _s=s, ** kargs).float())\n",
                "\n",
                "    return r\n",
                "\n",
                "\n",
                "def fetch_semantic_features(\n",
                "    types=[\"REAL\", \"NT\", \"DF\", \"FS\", \"F2F\"],\n",
                "    subjects=['q', 'k', 'v', 'out'],\n",
                "    seconds=1,\n",
                "    frames=2,\n",
                "    patch_num=14,\n",
                "    sample_num=100,\n",
                "    save_path=\"\",\n",
                "    seed=None,\n",
                "    visualize=False,\n",
                "    vpt_weight_path=\"\",\n",
                "    farl_weight_path=\"\",\n",
                "):\n",
                "    # the function summerizes the patch value at specific locations with semantic meanings.\n",
                "    assert not (len(vpt_weight_path) > 0 and len(farl_weight_path) > 0)\n",
                "\n",
                "    # create dataset & models\n",
                "    c = FFPP.get_default_config()\n",
                "    c.augmentation = \"none\"\n",
                "    c.random_speed = False\n",
                "    c.compressions = [\"c23\"]\n",
                "    c.types = types\n",
                "\n",
                "    accelerator = Accelerator(mixed_precision='no')\n",
                "\n",
                "    encoder = CLIP.load(\"ViT-B/16\")[0]\n",
                "    if len(farl_weight_path) > 0:\n",
                "        encoder.load_state_dict(\n",
                "            torch.load(farl_weight_path)[\"state_dict\"],\n",
                "            strict=False\n",
                "        )\n",
                "\n",
                "    encoder = encoder.visual.float()\n",
                "    if len(vpt_weight_path) > 0:\n",
                "        encoder.load_state_dict({\n",
                "            k[8:]: v for k, v in torch.load(vpt_weight_path, \"cpu\").items() if \"encoder\" == k[:7]\n",
                "        })\n",
                "\n",
                "    n_px = encoder.input_resolution\n",
                "\n",
                "    transform = T.Compose(\n",
                "        [\n",
                "            T.Resize(n_px, interpolation=T.InterpolationMode.BICUBIC),\n",
                "            T.CenterCrop(n_px),\n",
                "            T.ConvertImageDtype(torch.float32),\n",
                "            T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
                "                        (0.26862954, 0.26130258, 0.27577711)),\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    dataset = FFPP(\n",
                "        c.clone(),\n",
                "        frames,\n",
                "        seconds,\n",
                "        T.Compose([\n",
                "            T.Resize(n_px, interpolation=T.InterpolationMode.BICUBIC)\n",
                "        ]),\n",
                "        accelerator,\n",
                "        split=\"train\"\n",
                "    )\n",
                "\n",
                "    # the following patch locations are based on a 14x14 grid.\n",
                "    semantic_locations = {\n",
                "        \"eyes\": [\n",
                "            [4, 3], [4, 4], [4, 9], [4, 10]\n",
                "        ],\n",
                "        \"nose\": [\n",
                "            [7, 6], [6, 6], [5, 6],\n",
                "            [7, 7], [6, 7], [5, 7]\n",
                "        ],\n",
                "        \"lips\": [\n",
                "            [10, 5], [10, 6], [10, 7], [10, 8]\n",
                "        ],\n",
                "        \"eyebrows\": [\n",
                "            [2, 3], [2, 4],\n",
                "            [2, 9], [2, 10]\n",
                "        ],\n",
                "        \"skin\": [\n",
                "            [0, 6], [0, 7],\n",
                "            [1, 6], [1, 7],\n",
                "            [7, 3], [7, 4], [7, 9], [7, 10],\n",
                "            [12, 6], [12, 7],\n",
                "            # [13, 6], [13, 7]\n",
                "        ]\n",
                "    }\n",
                "\n",
                "    # part keypoint data, prepared for frame augmentations\n",
                "    part_keypoints = []\n",
                "    part_labels = []\n",
                "\n",
                "    for part in semantic_locations:\n",
                "        for loc in semantic_locations[part]:\n",
                "            part_keypoints.append(loc)\n",
                "            part_labels.append(part)\n",
                "    part_keypoints = (np.array(part_keypoints) + 0.5) / 14 * n_px\n",
                "\n",
                "    # frame augmentations\n",
                "    augmentations = alb.Compose(\n",
                "        [\n",
                "            alb.Flip(p=1.0),\n",
                "            alb.RandomResizedCrop(\n",
                "                n_px, n_px,\n",
                "                scale=(0.3, 0.7), ratio=(1, 1),\n",
                "                p=1.0,\n",
                "            )\n",
                "        ],\n",
                "        keypoint_params=alb.KeypointParams(format='yx', remove_invisible=True, label_fields=[\"part_name\"])\n",
                "    )\n",
                "\n",
                "    # container\n",
                "    layer_num = len(encoder.transformer.resblocks)\n",
                "    semantic_patches = {\n",
                "        s: {\n",
                "            k: [[] for _ in range(layer_num)]\n",
                "            for k in semantic_locations.keys()\n",
                "        }\n",
                "        for s in subjects\n",
                "    }\n",
                "\n",
                "    if (seed):\n",
                "        random.seed(seed)\n",
                "\n",
                "    # random samples\n",
                "    for _ in range(sample_num):\n",
                "        ########## random select index ############\n",
                "        idx = random.randint(0, len(dataset))\n",
                "        data = dataset[idx]\n",
                "\n",
                "        ######### extract video features ###########\n",
                "        for k, clip in data[0].items():\n",
                "            # sample augmentation\n",
                "            frame = clip[0].permute((1, 2, 0)).numpy()\n",
                "            result = augmentations(image=frame, keypoints=part_keypoints, part_name=part_labels)\n",
                "            rrc_frame, rrc_kp, rrc_lb = result[\"image\"], result[\"keypoints\"], result[\"part_name\"]\n",
                "\n",
                "            rrc_semantic_locations = {\n",
                "                part: []\n",
                "                for part in semantic_locations.keys()\n",
                "            }\n",
                "\n",
                "            for loc, part in zip(rrc_kp, rrc_lb):\n",
                "                rrc_semantic_locations[part].append(loc)\n",
                "\n",
                "            for part in rrc_semantic_locations.keys():\n",
                "                rrc_semantic_locations[part] = np.round(\n",
                "                    np.clip(np.array(rrc_semantic_locations[part]) / n_px * 14 - 0.5, a_min=0, a_max=14)\n",
                "                )\n",
                "\n",
                "            if visualize:\n",
                "                # visualization for debugging\n",
                "                # > visualize ordinary frame\n",
                "                plt.imshow(frame)\n",
                "                plt.scatter(part_keypoints[..., 1], part_keypoints[..., 0])\n",
                "                plt.show()\n",
                "                # > visualize frame after augmentation\n",
                "                rrc_kp = np.array(rrc_kp)\n",
                "                plt.imshow(rrc_frame)\n",
                "                plt.scatter(rrc_kp[..., 1], rrc_kp[..., 0])\n",
                "                plt.show()\n",
                "                # > visualize frame and keypoints at patch level\n",
                "                plt.imshow(cv2.resize(rrc_frame, (14, 14)))\n",
                "                for part in rrc_semantic_locations.keys():\n",
                "                    if (len(rrc_semantic_locations[part]) > 0):\n",
                "                        plt.scatter(rrc_semantic_locations[part][..., 1], rrc_semantic_locations[part][..., 0])\n",
                "                plt.show()\n",
                "\n",
                "            # extract frame features\n",
                "            features = extract_features(\n",
                "                encoder,\n",
                "                transform(\n",
                "                    torch.from_numpy(rrc_frame).permute((2, 0, 1)).unsqueeze(0)\n",
                "                )\n",
                "            )\n",
                "\n",
                "            # post-process semantic locations\n",
                "            rrc_semantic_locations = {\n",
                "                k: [\n",
                "                    int(_v[0] / 13 * (patch_num - 1)) * patch_num + int(_v[1] / 13 * (patch_num - 1))\n",
                "                    for _v in v\n",
                "                ]\n",
                "                for k, v in rrc_semantic_locations.items()\n",
                "            }\n",
                "\n",
                "            ######### extract video features ###########\n",
                "            for l in range(layer_num):\n",
                "                for s in subjects:\n",
                "                    for p, loc in rrc_semantic_locations.items():\n",
                "                        semantic_patches[s][p][l].extend(\n",
                "                            features[l][s][0, loc].tolist()\n",
                "                        )\n",
                "\n",
                "    semantic_patches = {\n",
                "        s: {\n",
                "            p: [\n",
                "                torch.tensor(semantic_patches[s][p][l]).mean(dim=0)\n",
                "                for l in range(layer_num)\n",
                "            ]\n",
                "            for p in semantic_locations.keys()\n",
                "        }\n",
                "        for s in subjects\n",
                "    }\n",
                "\n",
                "    if (len(save_path) > 0):\n",
                "        with open(save_path, \"wb\") as f:\n",
                "            pickle.dump(semantic_patches, f)\n",
                "\n",
                "    return semantic_patches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# all_semantic_features = fetch_semantic_features(sample_num=100)\n",
                "# semantic_features = fetch_semantic_features(\n",
                "#     [\"REAL\"],\n",
                "#     sample_num=1000,\n",
                "#     save_path=\"./misc/real_semantic_patches_1000.pickle\"\n",
                "# )\n",
                "\n",
                "# semantic_features = fetch_semantic_features(\n",
                "#     vpt_encoder=\"logs/test/olive-water-1118/best_weights.pt\",\n",
                "#     sample_num=300,\n",
                "#     save_path=\"./misc/ow_semantic_patch.pickle\"\n",
                "# )\n",
                "\n",
                "# semantic_features = fetch_semantic_features(\n",
                "#     farl_weight_path=\"./misc/FaRL-Base-Patch16-LAIONFace20M-ep64.pth\",\n",
                "#     sample_num=1000,\n",
                "#     save_path=\"./misc/farl_semantic_patches_v3.pickle\"\n",
                "# )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# real_semantic_features = fetch_semantic_features([\"REAL\"], sample_num=100, seed=10)\n",
                "# df_semantic_features = fetch_semantic_features([\"DF\"], sample_num=100, seed=10)\n",
                "# fs_semantic_features = fetch_semantic_features([\"FS\"], sample_num=100, seed=10)\n",
                "# f2f_semantic_features = fetch_semantic_features([\"F2F\"], sample_num=100, seed=10)\n",
                "# nt_semantic_features = fetch_semantic_features([\"NT\"], sample_num=100, seed=10)\n",
                "# all_semantic_features = fetch_semantic_features(sample_num=100, seed=10)\n",
                "\n",
                "# name_features = {\"REAL\": real_semantic_features, \"DF\": df_semantic_features, \"FS\": fs_semantic_features,\n",
                "#                  \"F2F\": f2f_semantic_features, \"NT\": nt_semantic_features, \"ALL\": all_semantic_features}\n",
                "# for subject in real_semantic_features.keys():\n",
                "#     for part in real_semantic_features[subject].keys():\n",
                "#         # for layer in range(len(real_semantic_features[subject][part])):\n",
                "#         for layer in [10, 11]:\n",
                "#             print(f\"===S:{subject} L:{layer} P:{part.upper()}===\")\n",
                "#             part_features = torch.stack([name_features[k][subject][part][layer] for k in name_features])\n",
                "#             score = torch.nn.functional.cosine_similarity(\n",
                "#                 part_features.unsqueeze(0),\n",
                "#                 part_features.unsqueeze(1),\n",
                "#                 dim=-1\n",
                "#             )\n",
                "#             print(list(name_features.keys()))\n",
                "#             print(f\"min:{torch.min(score)}\")\n",
                "#             print(score)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # create dataset & models\n",
                "# c = FFPP.get_default_config()\n",
                "# c.augmentation = \"none\"\n",
                "# c.random_speed = True\n",
                "# c.compressions = [\"c23\"]\n",
                "# c.types = [\"REAL\", \"DF\", \"FS\", \"F2F\", \"NT\"]\n",
                "# frames = 2\n",
                "# seconds = 4\n",
                "# accelerator = Accelerator(mixed_precision='no')\n",
                "# encoder = CLIP.load(\"ViT-B/16\")[0].visual.float()\n",
                "# n_px = encoder.input_resolution\n",
                "\n",
                "# transform = T.Compose(\n",
                "#     [\n",
                "#         T.RandomVerticalFlip(),\n",
                "#         T.RandomResizedCrop(n_px, (0.3, 0.5), (1, 1), interpolation=T.InterpolationMode.BICUBIC),\n",
                "#         # T.Resize(n_px, interpolation=T.InterpolationMode.BICUBIC),\n",
                "#         T.CenterCrop(n_px),\n",
                "#         T.ConvertImageDtype(torch.float32),\n",
                "#         T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
                "#                     (0.26862954, 0.26130258, 0.27577711)),\n",
                "#     ]\n",
                "# )\n",
                "\n",
                "# dataset = FFPP(c.clone(), frames, seconds, transform, accelerator, split=\"train\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# fetch_semantic_features(sample_num=10, visualize=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# data = dataset[random.randint(0, len(dataset))]\n",
                "# features = extract_features(encoder, data[0][\"c23\"])\n",
                "\n",
                "# evals = driver(features, var)\n",
                "# plotter(evals, \"\", \"subject-layer\", unit_size=2)\n",
                "\n",
                "\n",
                "# evals = driver(features, one_patch_cos_sim, t=0, c=21)\n",
                "# plotter(evals, \"\", \"layer-frame\", unit_size=2)\n",
                "\n",
                "\n",
                "# evals = driver(features, semantic_patch_cos_sim, part=\"lips\", s=\"k\", semantic_patches=semantic_patches)\n",
                "# plotter(evals, \"\", \"layer-frame\", unit_size=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# semantic_patches = fetch_semantic_features(sample_num=500)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# data = dataset[random.randint(0, len(dataset))]\n",
                "# features = extract_features(encoder, data[0][\"c23\"])\n",
                "# evals = driver(features, semantic_patch_cos_sim, part=\"eyes\", s=\"k\", semantic_patches=semantic_patches)\n",
                "# frame = data[0][\"c23\"][0]\n",
                "# frame = (frame.permute((1, 2, 0)) - frame.min())/(frame.max()-frame.min()).numpy()\n",
                "# print(frame.max())\n",
                "# plt.imshow(frame)\n",
                "# plt.show()\n",
                "# plotter(evals, \"\", \"layer-frame\", unit_size=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder = CLIP.load(\"ViT-B/16\")[0]\n",
                "# TODO: remove FaRL\n",
                "# encoder.load_state_dict(\n",
                "#     torch.load(\"./misc/FaRL-Base-Patch16-LAIONFace20M-ep64.pth\")[\"state_dict\"],\n",
                "#     strict=False\n",
                "# )\n",
                "encoder = encoder.visual.float()\n",
                "n_px = encoder.input_resolution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from tqdm import tqdm\n",
                "\n",
                "\n",
                "def sensitivity_for_perturbations(\n",
                "    encoder,\n",
                "    n_px,\n",
                "    num_samples,\n",
                "    attributes=['q', 'k', 'v', 'out'],\n",
                "    augmentation=\"perturbations\",\n",
                "):\n",
                "    c = FFPP.get_default_config()\n",
                "    c.random_speed = False\n",
                "    c.compressions = [\"c23\"]\n",
                "    c.types = [\"REAL\", \"DF\", \"FS\", \"F2F\", \"NT\"]\n",
                "    frames = 1\n",
                "    seconds = 3\n",
                "    accelerator = Accelerator(mixed_precision='no')\n",
                "\n",
                "    transform = T.Compose(\n",
                "        [\n",
                "            T.Resize(n_px),\n",
                "            T.CenterCrop(n_px),\n",
                "            T.ConvertImageDtype(torch.float32),\n",
                "            T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
                "                        (0.26862954, 0.26130258, 0.27577711)),\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    c.augmentation = \"none\"\n",
                "    dataset1 = FFPP(c.clone(), frames, seconds, transform, accelerator, split=\"train\")\n",
                "    c.augmentation = augmentation\n",
                "    dataset2 = FFPP(c.clone(), frames, seconds, transform, accelerator, split=\"train\")\n",
                "\n",
                "    # storage for patch-wise cosine distance of attention attributes\n",
                "    storage = [{k: torch.zeros(196) for k in attributes} for _ in range(12)]\n",
                "\n",
                "    for i in tqdm(range(num_samples)):\n",
                "        idx = random.randrange(0, len(dataset1))\n",
                "        data1 = dataset1[idx]\n",
                "        data2 = dataset2[idx]\n",
                "        #######################################\n",
                "        # plt.figure(figsize=(50, 5))\n",
                "        # plt.subplot(2, 1, 1)\n",
                "        # plt.imshow(\n",
                "        #     np.stack(\n",
                "        #         (data1[0][\"c23\"][:30]).numpy().transpose((0, 2, 3, 1)), axis=1\n",
                "        #     ).reshape((n_px, -1, 3))\n",
                "        # )\n",
                "        # plt.subplot(2, 1, 2)\n",
                "        # plt.imshow(\n",
                "        #     np.stack(\n",
                "        #         (data2[0][\"c23\"][:30]).numpy().transpose((0, 2, 3, 1)), axis=1\n",
                "        #     ).reshape((n_px, -1, 3))\n",
                "        # )\n",
                "        #######################################\n",
                "        features1 = extract_features(encoder, data1[0][\"c23\"])\n",
                "        features2 = extract_features(encoder, data2[0][\"c23\"])\n",
                "        for i in range(12):\n",
                "            for attr in attributes:\n",
                "                storage[i][attr] += (\n",
                "                    1 - torch.nn.functional.cosine_similarity(\n",
                "                        features1[i][attr], features2[i][attr], dim=-1\n",
                "                    )\n",
                "                ).squeeze(0) / 2 / num_samples\n",
                "    return storage\n",
                "\n",
                "\n",
                "target_folder = \"./misc/attn_attr_sens/\"\n",
                "os.makedirs(target_folder, exist_ok=True)\n",
                "scenario_storages = {}\n",
                "for aug_type in [\n",
                "    \"perturbations\",\n",
                "    \"dev-mode+force-rgb\",\n",
                "    \"dev-mode+force-hue\",\n",
                "    \"dev-mode+force-bright\",\n",
                "    \"dev-mode+force-comp\",\n",
                "    \"dev-mode+force-dscale\",\n",
                "    \"dev-mode+force-sharpen\",\n",
                "]:\n",
                "    scenario_storages[aug_type] = sensitivity_for_perturbations(\n",
                "        encoder,\n",
                "        n_px,\n",
                "        100,\n",
                "        augmentation=aug_type\n",
                "    )\n",
                "\n",
                "\n",
                "# find max & min\n",
                "global_max = -10\n",
                "global_min = 10\n",
                "for storage in scenario_storages.values():\n",
                "    for i, attrs in enumerate(storage):\n",
                "        for j, attr in enumerate(attrs):\n",
                "            # record\n",
                "            global_min = min(storage[i][attr].min().item(), global_min)\n",
                "            global_max = max(storage[i][attr].max().item(), global_max)\n",
                "\n",
                "for aug_type, storage in scenario_storages.items():\n",
                "    plt.figure(figsize=(len(storage) * 1, len(storage[0]) * 1), layout=\"constrained\")\n",
                "    for i, attrs in enumerate(storage):\n",
                "        for j, attr in enumerate(attrs):\n",
                "            plt.subplot(len(storage[0]), len(storage), j * len(storage) + i + 1)\n",
                "            data = (storage[i][attr].view(14, 14).numpy() - global_min) / (global_max - global_min)\n",
                "            plt.imshow(data, vmin=0, vmax=1)\n",
                "            plt.gca().axis(\"off\")\n",
                "    plt.savefig(os.path.join(target_folder, f\"{aug_type}.pdf\"))\n",
                "    plt.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "dfd-clip",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.16"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "c9d400f0712968c6b0c2c185442708fcff6ca365f2120d94e1d89a9df5bd30ab"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
