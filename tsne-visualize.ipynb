{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import statistics\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import src.clip as clip\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from src.models import Detector, DINOv2, remap_weight\n",
    "from sklearn.manifold import TSNE\n",
    "import torchvision.transforms as T\n",
    "from matplotlib import pyplot as plt\n",
    "from accelerate import Accelerator\n",
    "from yacs.config import CfgNode as CN\n",
    "from main import get_config, init_accelerator, set_seed, FFPP, CDF, DFDC\n",
    "# logging.basicConfig(level=\"DEBUG\")\n",
    "\n",
    "\n",
    "class Obj:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch frames for each df_type\n",
    "def generate_sample(num_samples_per_df_type=100, sample_frame_indices=[0, 1, 2], clip_duration=8, clip_frames=4, folder=\"frames\", backbone=\"clip\"):\n",
    "    df_type_list = [\"REAL\", \"NT\", \"DF\", \"FS\", \"F2F\"]\n",
    "    c = FFPP.get_default_config()\n",
    "    c.augmentation = \"normal+frame\"\n",
    "    c.compressions = [\"c23\"]\n",
    "    c.types = []\n",
    "\n",
    "    accelerator = Accelerator(mixed_precision='no')\n",
    "    if backbone == \"clip\":\n",
    "        model = clip.load(\"ViT-B/16\")[0].visual.float()\n",
    "        model.eval()\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.Resize(model.input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.CenterCrop(model.input_resolution),\n",
    "            T.ConvertImageDtype(torch.float32),\n",
    "            T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                        (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "    elif backbone == \"dino\":\n",
    "        model = DINOv2()\n",
    "        model.eval()\n",
    "\n",
    "        transform = T.Compose([\n",
    "            T.Resize(model.input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.CenterCrop(model.input_resolution),\n",
    "            T.ConvertImageDtype(torch.float32),\n",
    "            T.Normalize(\n",
    "                (0.485, 0.456, 0.406),\n",
    "                (0.229, 0.224, 0.225)\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    for df_type in df_type_list:\n",
    "        base_dir = f\"./misc/{folder}/{df_type}/\"\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        c.types = [df_type]\n",
    "        x = FFPP(c.clone(), clip_frames, clip_duration, transform, accelerator, split=\"train\")\n",
    "        while (int(num_samples_per_df_type * len(sample_frame_indices)) > len(glob(os.path.join(base_dir, \"*\")))):\n",
    "            c_idx = random.randrange(0, len(x))\n",
    "            frames = x[c_idx][0][\"c23\"]\n",
    "            for i in sample_frame_indices:\n",
    "                torch.save(frames[i], os.path.join(base_dir, f\"{c_idx}_{str(i).zfill(3)}.pt\"))\n",
    "\n",
    "\n",
    "# fetch CDF/DFDC dataset frames for each label\n",
    "def generate_cross_sample(num_samples_per_df_type=100, sample_frame_indices=[0, 1, 2], clip_duration=8, clip_frames=4, folder=\"frames\", backbone=\"clip\"):\n",
    "    for dataset_cls in [DFDC, CDF]:\n",
    "        c = dataset_cls.get_default_config()\n",
    "\n",
    "        accelerator = Accelerator(mixed_precision='no')\n",
    "        if backbone == \"clip\":\n",
    "            model = clip.load(\"ViT-B/16\")[0].visual.float()\n",
    "            model.eval()\n",
    "\n",
    "            transform = T.Compose([\n",
    "                T.Resize(model.input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "                T.CenterCrop(model.input_resolution),\n",
    "                T.ConvertImageDtype(torch.float32),\n",
    "                T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                            (0.26862954, 0.26130258, 0.27577711)),\n",
    "            ])\n",
    "        elif backbone == \"dino\":\n",
    "            model = DINOv2()\n",
    "            model.eval()\n",
    "\n",
    "            transform = T.Compose([\n",
    "                T.Resize(model.input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "                T.CenterCrop(model.input_resolution),\n",
    "                T.ConvertImageDtype(torch.float32),\n",
    "                T.Normalize(\n",
    "                    (0.485, 0.456, 0.406),\n",
    "                    (0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ])\n",
    "\n",
    "        for label in [\"REAL\", \"FAKE\"]:\n",
    "            base_dir = f\"./misc/{folder}/{dataset_cls.__name__}_{label}/\"\n",
    "            os.makedirs(base_dir, exist_ok=True)\n",
    "            x = dataset_cls(c.clone(), clip_frames, clip_duration, transform, accelerator, split=\"test\")\n",
    "            while (int(num_samples_per_df_type * len(sample_frame_indices)) > len(glob(os.path.join(base_dir, \"*\")))):\n",
    "                c_idx = random.randrange(0, len(x))\n",
    "                if not x.video_info(c_idx)[1] == label:\n",
    "                    continue\n",
    "                frames = x[c_idx][0]\n",
    "                for i in sample_frame_indices:\n",
    "                    torch.save(frames[i], os.path.join(base_dir, f\"{c_idx}_{str(i).zfill(3)}.pt\"))\n",
    "\n",
    "# generate_sample(100,[0,1,2],1,20,\"extern/dino_1s20f/\",\"dino\")\n",
    "# generate_cross_sample(100,[0,1,2],8,4,\"extern/clip/8s4f/frames/\",\"clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_kvs(\n",
    "    glob_exp=\"*/*\",\n",
    "    folder=\"features\",\n",
    "    frame_dir=\"frames\",\n",
    "    backbone=\"clip\",\n",
    "    vpt_encoder=\"logs/test/magic-hill-770/best_weights.pt\"\n",
    "):\n",
    "    if (backbone == \"clip\"):\n",
    "        model = clip.load(\"ViT-B/16\")[0].visual.float()\n",
    "        model.load_state_dict({\n",
    "            k[8:]: v for k, v in torch.load(vpt_encoder, \"cpu\").items() if \"encoder\" == k[:7]\n",
    "        })\n",
    "    elif (backbone == \"dino\"):\n",
    "        assert False, \"DINO is not ready for the VPT.\"\n",
    "        model = DINOv2()\n",
    "    model.eval()\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    base_dir = f\"./misc/{folder}/\"\n",
    "    for file in tqdm(glob(os.path.join(base_dir.replace(folder, frame_dir), glob_exp))):\n",
    "        target = file.replace(frame_dir, folder)\n",
    "        if (os.path.exists(target)):\n",
    "            continue\n",
    "        os.makedirs(os.path.split(target)[0], exist_ok=True)\n",
    "        frame = torch.load(file, \"cpu\")\n",
    "        if backbone == \"clip\":\n",
    "            kvs = model(frame.unsqueeze(0).to(\"cuda\"), with_q=True, with_out=True)\n",
    "        elif backbone == \"dino\":\n",
    "            kvs = model(frame.unsqueeze(0).to(\"cuda\"), feat_keys=[\"q\", \"k\", \"v\", \"out\"])\n",
    "        torch.save(kvs, target)\n",
    "        del kvs\n",
    "\n",
    "\n",
    "# generate_kvs(\n",
    "#     glob_exp=\"*/*\",\n",
    "#     folder=\"extern/dino_1s20f/features/\",\n",
    "#     frame_dir=\"extern/dino_1s20f/frames/\",\n",
    "#     backbone=\"dino\"\n",
    "# )\n",
    "# generate_kvs(\n",
    "#     glob_exp=\"*/*\",\n",
    "#     folder=\"extern/clip_vpt(50,all)/1s20f/features/\",\n",
    "#     frame_dir=\"extern/clip/1s20f/frames/\",\n",
    "#     backbone=\"clip\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(embeddings, labels, perplexities, graph_title, save_path, save=False):\n",
    "    X = np.array(embeddings)\n",
    "    plt.figure(figsize=(5 * len(perplexities), 4), layout=\"constrained\")\n",
    "    plt.suptitle(graph_title)\n",
    "    for i, perplexity in enumerate(perplexities):\n",
    "        print(f\"TSNE Running(Perplexity:{perplexity})....\")\n",
    "        tsne = TSNE(n_components=2, n_iter=10000, learning_rate='auto', init='random', perplexity=perplexity)\n",
    "        _X = tsne.fit_transform(X)\n",
    "        print(f\"TSNE Completed.\")\n",
    "\n",
    "        color = {\n",
    "            \"REAL\": \"green\",\n",
    "            \"DF\": \"blue\",\n",
    "            \"FS\": \"purple\",\n",
    "            \"F2F\": \"darkorange\",\n",
    "            \"NT\": \"red\",\n",
    "            \"CDF_REAL\": \"turquoise\",\n",
    "            \"CDF_FAKE\": \"deeppink\",\n",
    "            \"DFDC_REAL\": \"forestgreen\",\n",
    "            \"DFDC_FAKE\": \"steelblue\"\n",
    "        }\n",
    "\n",
    "        plt.subplot(1, len(perplexities), i + 1)\n",
    "        plt.title(f\"Perplexity:{perplexity}\")\n",
    "        plt.gca().set_xticks([])\n",
    "        plt.gca().set_yticks([])\n",
    "\n",
    "        offset = 0\n",
    "        for category_type, num in labels:\n",
    "            print(category_type, num, offset)\n",
    "            plt.scatter(\n",
    "                _X[offset:offset + num, 0],\n",
    "                _X[offset:offset + num, 1],\n",
    "                3,\n",
    "                # color=\"green\" if category_type == \"REAL\" else \"red\",\n",
    "                color=color[category_type],\n",
    "                label=category_type if i == 0 else \"\",\n",
    "                alpha=0.5\n",
    "            )\n",
    "            offset += num\n",
    "\n",
    "        print(\"Rationality Check:\", offset == len(_X))\n",
    "\n",
    "    plt.gcf().legend(loc='outside right center')\n",
    "    if (save):\n",
    "        folder, file = os.path.split(save_path)\n",
    "        if (not os.path.exists(folder)):\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector(\n",
    "        path,\n",
    "        weight_name=\"best_weights.pt\"\n",
    "):\n",
    "    cfg_path = path\n",
    "    with open(cfg_path) as f:\n",
    "        preset = CN(yaml.safe_load(f))\n",
    "\n",
    "    mc = Detector.get_default_config().merge_from_other_cfg(preset.model)\n",
    "\n",
    "    accelerator = Accelerator(mixed_precision=preset.system.mixed_precision)\n",
    "    model = Detector(mc, preset.data.num_frames, accelerator)\n",
    "    weights = remap_weight(\n",
    "        model,\n",
    "        torch.load(\n",
    "            os.path.join(os.path.split(cfg_path)[0], weight_name),\n",
    "            map_location=\"cpu\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(\n",
    "        weights\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(accelerator.device)\n",
    "    return model, accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_single(\n",
    "    feature_folder,\n",
    "    ratio=1.0,\n",
    "    perplexities=[20, 50, 80],\n",
    "    patch_num=14,\n",
    "    save=False,\n",
    "    save_folder=\"./misc/graphs/test/\",\n",
    "    glob_exp=\"*_000.*\",\n",
    "    num_frames_per_sample=1\n",
    "):\n",
    "\n",
    "    category_type_list = [d for d in os.listdir(feature_folder) if os.path.isdir(os.path.join(feature_folder, d))]\n",
    "\n",
    "    for target_layer in range(0, 12):\n",
    "        print(f\"Processing  Layer {target_layer}\")\n",
    "\n",
    "        layer_features = {\n",
    "            i: {t: [] for t in [\"q\", \"out\", \"k\", \"v\"]}\n",
    "            for i in category_type_list\n",
    "        }\n",
    "\n",
    "        for df_type in layer_features.keys():\n",
    "            base_dir = os.path.join(feature_folder, df_type)\n",
    "            files = sorted(glob(os.path.join(base_dir, glob_exp)))\n",
    "            files = files[:int(len(files) * ratio // num_frames_per_sample * num_frames_per_sample)]\n",
    "\n",
    "            print(\"Files:\", files)\n",
    "            print(\"Length:\", len(files))\n",
    "\n",
    "            for file in files:\n",
    "                features = torch.load(file, \"cpu\")[target_layer]\n",
    "                for subject in layer_features[df_type].keys():\n",
    "                    # cls only\n",
    "                    # print(features[subject][:,0].shape)\n",
    "                    # return\n",
    "                    # layer_features[df_type][subject] += features[subject][:, 0].view((-1, 768)).half().tolist()\n",
    "                    # without cls\n",
    "                    # layer_features[df_type][subject] += features[subject][:,1:].view((-1, 768)).half().tolist()\n",
    "                    # all\n",
    "                    # layer_features[df_type][subject] += features[subject].view((-1, 768)).half().tolist()\n",
    "                    # crop center\n",
    "                    # layer_features[df_type][subject] += features[subject][:,1:].view((-1,14,14,768))[:,4:11,4:11,:].reshape((-1,768)).half().tolist()\n",
    "                    # flatten\n",
    "                    # layer_features[df_type][subject] += features[subject].half().view((1,-1)).tolist()\n",
    "                    # crop center flatten\n",
    "                    qaurt_patch = patch_num // 4\n",
    "                    layer_features[df_type][subject].append(features[subject][:, 1:].view(\n",
    "                        (-1, patch_num, patch_num, 768))[:, qaurt_patch:-qaurt_patch, qaurt_patch:-qaurt_patch, :].flatten().tolist()\n",
    "                    )\n",
    "\n",
    "        print(f\"Layer {target_layer} Features Loaded.\")\n",
    "        for subject in [\"q\", \"out\", \"k\", \"v\"]:\n",
    "            print(f\"Processing Subject {subject}.\")\n",
    "            labels = []\n",
    "            embeddings = []\n",
    "            for df_type in layer_features.keys():\n",
    "                features = layer_features[df_type][subject]\n",
    "                labels.append((df_type, len(features)))\n",
    "                embeddings += features\n",
    "\n",
    "            print(f\"Embeddings Built.\")\n",
    "            tsne(\n",
    "                embeddings,\n",
    "                labels,\n",
    "                perplexities,\n",
    "                f\"Layer={target_layer},Subject={subject},Ratio={ratio}\",\n",
    "                os.path.join(save_folder, f\"L{target_layer}S{subject}.pdf\"),\n",
    "                save\n",
    "            )\n",
    "\n",
    "\n",
    "tsne_single(\"./misc/extern/clip/vpt/1s20f/features/\", save=True, save_folder=\"./misc/graphs/vpt_1s20f_single_ccrop/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP EXCLUSIVE\n",
    "# def tsne_adapter():\n",
    "#     torch.cuda.empty_cache()\n",
    "#     ratio = 1.0\n",
    "#     glob_exp = \"*_0.*\"\n",
    "#     image_folder = \"./misc/graphs/test/\"\n",
    "#     feature_folder = \"./misc/1s20f/features/\"\n",
    "#     num_frames_per_sample = 1\n",
    "#     perplexities = [20, 50, 80]\n",
    "\n",
    "#     os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "#     df_type_list = [\"REAL\", \"DF\"]\n",
    "#     subjects = [\"k\", \"v\"]\n",
    "#     layer_features = [\n",
    "#         {\n",
    "#             i: {t: [] for t in [\"k\", \"v\"]}\n",
    "#             for i in df_type_list\n",
    "#         }\n",
    "#         for _ in range(6)\n",
    "#     ]\n",
    "\n",
    "#     # load adapter from model\n",
    "#     model, _ = get_detector()\n",
    "#     adapter = model.adapter\n",
    "\n",
    "#     for df_type in df_type_list:\n",
    "#         base_dir = os.path.join(feature_folder, df_type)\n",
    "#         files = sorted(glob(os.path.join(base_dir, glob_exp)))\n",
    "#         files = files[:int(len(files) * ratio // num_frames_per_sample * num_frames_per_sample)]\n",
    "\n",
    "#         print(\"Files:\", files)\n",
    "#         print(\"Length:\", len(files))\n",
    "\n",
    "#         for file in files:\n",
    "#             features = torch.load(file, \"cpu\")[6:]\n",
    "\n",
    "#             # drop unrequired features\n",
    "#             for i in range(len(features)):\n",
    "#                 features[i].pop(\"q\")\n",
    "#                 features[i].pop(\"out\")\n",
    "#                 features[i][\"v\"] = features[i][\"v\"][:, 1:].view(1, 1, -1, 12, 64).to(\"cuda\")\n",
    "#                 features[i][\"k\"] = features[i][\"k\"][:, 1:].view(1, 1, -1, 12, 64).to(\"cuda\")\n",
    "\n",
    "#             # adapt features\n",
    "#             features = adapter(features)\n",
    "\n",
    "#             # save adapted features\n",
    "#             for i in range(len(features)):\n",
    "#                 for subject in subjects:\n",
    "#                     layer_features[i][df_type][subject].append(\n",
    "#                         features[i][subject].flatten().tolist()\n",
    "#                     )\n",
    "\n",
    "#     for target_layer in range(6):\n",
    "#         for subject in subjects:\n",
    "#             print(f\"Processing Subject {subject}.\")\n",
    "#             labels = []\n",
    "#             embeddings = []\n",
    "#             for df_type in df_type_list:\n",
    "#                 features = layer_features[target_layer][df_type][subject]\n",
    "#                 labels.append((df_type, len(features)))\n",
    "#                 embeddings += features\n",
    "\n",
    "#             print(f\"Embeddings Built.\")\n",
    "#             tsne(\n",
    "#                 embeddings,\n",
    "#                 labels,\n",
    "#                 perplexities,\n",
    "#                 f\"Layer={target_layer},Subject={subject},Ratio={ratio}\",\n",
    "#                 os.path.join(image_folder, f\"L{target_layer}S{subject}.pdf\")\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_temporal(\n",
    "    feature_folder,\n",
    "    ratio=1.0,\n",
    "    temporal_indices=[0, 1],\n",
    "    alignment=False,\n",
    "    softmax_first=False,\n",
    "    spatial_size=7,\n",
    "    perplexities=[20, 50, 80],\n",
    "    patch_num=14,\n",
    "    save=False,\n",
    "    save_folder=\"./misc/graphs/test/\",\n",
    "    glob_exp=\"*\",\n",
    "    num_frames_per_sample=3\n",
    "):\n",
    "    quart_patch = patch_num // 4\n",
    "    assert spatial_size % 2 == 1\n",
    "\n",
    "    category_type_list = [d for d in os.listdir(feature_folder) if os.path.isdir(os.path.join(feature_folder, d))]\n",
    "\n",
    "    for target_layer in range(0, 12):\n",
    "        print(f\"Processing  Layer {target_layer}\")\n",
    "\n",
    "        layer_features = {\n",
    "            i: {\"attn\": []}\n",
    "            for i in category_type_list\n",
    "        }\n",
    "\n",
    "        for df_type in layer_features.keys():\n",
    "            base_dir = os.path.join(feature_folder, df_type)\n",
    "            files = sorted(glob(os.path.join(base_dir, glob_exp)))\n",
    "            files = files[:int(len(files) * ratio // num_frames_per_sample * num_frames_per_sample)]\n",
    "\n",
    "            print(\"Files:\", files)\n",
    "            print(\"Length:\", len(files))\n",
    "\n",
    "            for offset in range(0, len(files), num_frames_per_sample):\n",
    "\n",
    "                # fetch consecutive frames\n",
    "                vid_features = {\"k\": [], \"q\": []}\n",
    "                for temporal_index in temporal_indices:\n",
    "                    file = files[offset + temporal_index]\n",
    "                    features = torch.load(file, \"cpu\")[target_layer]\n",
    "                    for subject in vid_features.keys():\n",
    "                        vid_features[subject].append(features[subject][:, 1:].view((-1, 768)))\n",
    "\n",
    "                # stack consecutive accroding to the subject\n",
    "                for k in vid_features.keys():\n",
    "                    vid_features[k] = torch.stack(vid_features[k])\n",
    "\n",
    "                # perform cross-frame patch attention, reshape the attention scores into spatial form.\n",
    "                aff = torch.einsum(\n",
    "                    'qhd,khd->hqk',\n",
    "                    vid_features[\"q\"][0].unflatten(-1, (12, 64)) / (vid_features[\"q\"].shape[-1]**0.5),\n",
    "                    vid_features[\"k\"][-1].unflatten(-1, (12, 64))\n",
    "                )\n",
    "\n",
    "                if (softmax_first):\n",
    "                    aff = aff.softmax(dim=-1)\n",
    "\n",
    "                aff = aff.view((-1, patch_num, patch_num, patch_num, patch_num))\n",
    "\n",
    "                # fetch neighbor patch attention score of proximity frames\n",
    "                patch_features = []\n",
    "                for j in range(quart_patch, patch_num - quart_patch):\n",
    "                    for k in range(quart_patch, patch_num - quart_patch):\n",
    "\n",
    "                        if (alignment):\n",
    "                            half_size = spatial_size // 2\n",
    "                            att_feat = aff[:, j, k, j - half_size:j + half_size +\n",
    "                                           1, k - half_size:k + half_size + 1].flatten(1)\n",
    "                        else:\n",
    "                            att_feat = aff[:, j, k,].flatten(1)\n",
    "\n",
    "                        if not softmax_first:\n",
    "                            att_feat = att_feat.softmax(dim=-1)\n",
    "\n",
    "                        att_feat = att_feat.flatten().tolist()\n",
    "                        patch_features += att_feat\n",
    "\n",
    "                layer_features[df_type][\"attn\"].append(patch_features)\n",
    "\n",
    "        print(f\"Layer {target_layer} Features Loaded.\")\n",
    "        for subject in [\"attn\"]:\n",
    "            print(f\"Processing Subject {subject}.\")\n",
    "            labels = []\n",
    "            embeddings = []\n",
    "            for df_type in layer_features.keys():\n",
    "                features = layer_features[df_type][subject]\n",
    "                labels.append((df_type, len(features)))\n",
    "                embeddings += features\n",
    "            print(f\"Embeddings Built.\")\n",
    "\n",
    "            tsne(\n",
    "                embeddings,\n",
    "                labels,\n",
    "                perplexities,\n",
    "                f\"Layer={target_layer},Subject={subject},Ratio={ratio}\",\n",
    "                os.path.join(save_folder, f\"L{target_layer}S{subject}.pdf\"),\n",
    "                save\n",
    "            )\n",
    "\n",
    "            print(f\"Subject {subject} completed.\")\n",
    "\n",
    "# tsne_temporal(\"./misc/extern/dino_1s20f/features/\",patch_num=16)\n",
    "# tsne_temporal(\n",
    "#     \"./misc/extern/clip/1s20f/features/\",\n",
    "#     alignment=True,\n",
    "#     softmax_first=True,\n",
    "#     save=True,\n",
    "#     save_folder=\"./misc/graphs/1s20f_temporal_ccrop/\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_spatial(\n",
    "    feature_folder,\n",
    "    ratio=1.0,\n",
    "    normalize=False,\n",
    "    perplexities=[20, 50, 80],\n",
    "    patch_num=14,\n",
    "    save=False,\n",
    "    save_folder=\"./misc/graphs/test/\",\n",
    "    glob_exp=\"*_000.*\",\n",
    "    num_frames_per_sample=1\n",
    "):\n",
    "    category_type_list = [d for d in os.listdir(feature_folder) if os.path.isdir(os.path.join(feature_folder, d))]\n",
    "\n",
    "    for target_layer in range(0, 12):\n",
    "        print(f\"Processing  Layer {target_layer}\")\n",
    "\n",
    "        layer_features = {\n",
    "            i: {t: [] for t in [\"attn\"]}\n",
    "            for i in category_type_list\n",
    "        }\n",
    "\n",
    "        for df_type in layer_features.keys():\n",
    "            base_dir = os.path.join(feature_folder, df_type)\n",
    "            files = sorted(glob(os.path.join(base_dir, glob_exp)))\n",
    "            files = files[:int(len(files) * ratio // num_frames_per_sample * num_frames_per_sample)]\n",
    "\n",
    "            print(\"Files:\", files)\n",
    "            print(\"Length:\", len(files))\n",
    "\n",
    "            for file in files:\n",
    "                features = torch.load(file, \"cpu\")[target_layer]\n",
    "                frame_features = {\"q\": None, \"k\": None}\n",
    "                for subject in frame_features.keys():\n",
    "                    frame_features[subject] = features[subject][:, 1:].view((-1, 768))\n",
    "\n",
    "                aff = torch.einsum(\n",
    "                    'qh,kh->qk',\n",
    "                    frame_features[\"q\"] / (frame_features[\"q\"].shape[-1]**0.5),\n",
    "                    frame_features[\"k\"]\n",
    "                )\n",
    "\n",
    "                if (normalize):\n",
    "                    aff = aff.softmax(dim=-1)\n",
    "\n",
    "                quart_patch = patch_num // 4\n",
    "                layer_features[df_type][\"attn\"].append(\n",
    "                    aff.view(patch_num, patch_num, -1)[quart_patch:-quart_patch,\n",
    "                                                       quart_patch:-quart_patch].flatten().tolist()\n",
    "                )\n",
    "\n",
    "        print(f\"Layer {target_layer} Features Loaded.\")\n",
    "        for subject in [\"attn\"]:\n",
    "            print(f\"Processing Subject {subject}.\")\n",
    "            labels = []\n",
    "            embeddings = []\n",
    "            for df_type in layer_features.keys():\n",
    "                features = layer_features[df_type][subject]\n",
    "                labels.append((df_type, len(features)))\n",
    "                embeddings += features\n",
    "\n",
    "            print(f\"Embeddings Built.\")\n",
    "\n",
    "            tsne(\n",
    "                embeddings,\n",
    "                labels,\n",
    "                perplexities,\n",
    "                f\"Layer={target_layer},Subject={subject},Ratio={ratio}\",\n",
    "                os.path.join(save_folder, f\"L{target_layer}S{subject}.pdf\"),\n",
    "                save\n",
    "            )\n",
    "\n",
    "            print(f\"Subject {subject} completed.\")\n",
    "\n",
    "\n",
    "# tsne_spatial(\"./misc/extern/dino_1s20f/features/\", patch_num=16)\n",
    "# tsne_spatial(\n",
    "#     \"./misc/extern/clip/1s20f/features/\",\n",
    "#     normalize=True,\n",
    "#     save=True,\n",
    "#     save_folder=\"./misc/graphs/1s20f_spatial_ccrop2/\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tsne_semantic():\n",
    "#     import pickle\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     # load semantic features\n",
    "#     with open(\"misc/semantic_patches.pickle\", \"rb\") as f:\n",
    "#         semantic_queries = pickle.load(f)\n",
    "#     num_parts = len(semantic_queries[\"q\"].keys())\n",
    "#     ratio = 1.0\n",
    "#     glob_exp = \"*_0.*\"\n",
    "#     image_folder = \"./misc/graphs/test/\"\n",
    "#     feature_folder = \"./misc/1s20f/features/\"\n",
    "#     num_frames_per_sample = 1\n",
    "#     perplexities = [20, 50, 80]\n",
    "\n",
    "#     # vert_filter = torch.tensor([[[\n",
    "#     #     [-1, -2, -1],\n",
    "#     #     [0, 0, 0],\n",
    "#     #     [1, 2, 1]\n",
    "#     # ]]]).float().repeat(num_parts, 1, 1, 1)\n",
    "\n",
    "#     # hori_filter = torch.tensor([[[\n",
    "#     #     [-1, 0, 1],\n",
    "#     #     [-2, 0, 2],\n",
    "#     #     [-1, 0, 1]\n",
    "#     # ]]]).float().repeat(num_parts, 1, 1, 1)\n",
    "\n",
    "#     os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "#     patch_loc_embedding = torch.stack(\n",
    "#         [torch.randn(10) + 1 * i for i in range(196)]\n",
    "#     )\n",
    "\n",
    "#     for target_layer in range(0, 12):\n",
    "#         print(f\"Processing  Layer {target_layer}\")\n",
    "\n",
    "#         layer_features = {\n",
    "#             i: {t: [] for t in [\"attn\"]}\n",
    "#             for i in df_type_list\n",
    "#         }\n",
    "\n",
    "#         for df_type in layer_features.keys():\n",
    "#             base_dir = os.path.join(feature_folder, df_type)\n",
    "#             files = sorted(glob(os.path.join(base_dir, glob_exp)))\n",
    "#             files = files[:int(len(files) * ratio // num_frames_per_sample * num_frames_per_sample)]\n",
    "\n",
    "#             print(\"Files:\", files)\n",
    "#             print(\"Length:\", len(files))\n",
    "\n",
    "#             for file in files:\n",
    "#                 features = torch.load(file, \"cpu\")[target_layer]\n",
    "#                 frame_features = {\"q\": None, \"k\": None, \"v\": None, \"out\": None}\n",
    "#                 for subject in frame_features.keys():\n",
    "#                     frame_features[subject] = features[subject][:, 1:].view((-1, 768))\n",
    "\n",
    "#                 # semantic_layer_queries = torch.stack([\n",
    "#                 #     semantic_queries[\"q\"][part][target_layer] for part in semantic_queries[\"q\"].keys()\n",
    "#                 # ]).unflatten(-1,(12,64))\n",
    "\n",
    "#                 # aff = torch.einsum(\n",
    "#                 #     'qhd,khd->qkh',\n",
    "#                 #     # semantic_layer_queries/(semantic_layer_queries.shape[-1]**0.5),\n",
    "#                 #     semantic_layer_queries,\n",
    "#                 #     frame_features[\"k\"].unflatten(-1,(12,64))\n",
    "#                 # )\n",
    "\n",
    "#                 semantic_layer_queries = torch.stack([\n",
    "#                     semantic_queries[\"q\"][part][target_layer] for part in semantic_queries[\"q\"].keys()\n",
    "#                 ])\n",
    "#                 ########################\n",
    "#                 # aff = torch.einsum(\n",
    "#                 #     'qh,kh->qk',\n",
    "#                 #     semantic_layer_queries/(semantic_layer_queries.shape[-1]**0.5),\n",
    "#                 #     frame_features[\"k\"]\n",
    "#                 # ).softmax(dim=-1).view((1,-1,14,14))\n",
    "\n",
    "#                 # aff = F.conv2d(aff,vert_filter, padding=1,groups=num_parts) + F.conv2d(aff,hori_filter, padding=1,groups=num_parts)\n",
    "#                 ########################\n",
    "#                 aff = torch.einsum(\n",
    "#                     'qh,kh->qk',\n",
    "#                     semantic_layer_queries / (semantic_layer_queries.shape[-1]**0.5),\n",
    "#                     frame_features[\"k\"]\n",
    "#                 ).softmax(dim=-1)\n",
    "\n",
    "#                 aff = torch.einsum(\n",
    "#                     'qk,kn->qn',\n",
    "#                     aff,\n",
    "#                     patch_loc_embedding\n",
    "#                 )\n",
    "\n",
    "#                 # print(aff.shape)\n",
    "#                 # return\n",
    "#                 layer_features[df_type][\"attn\"].append(aff.flatten().tolist())\n",
    "\n",
    "#         print(f\"Layer {target_layer} Features Loaded.\")\n",
    "\n",
    "#         for subject in [\"attn\"]:\n",
    "#             print(f\"Processing Subject {subject}.\")\n",
    "#             labels = []\n",
    "#             embeddings = []\n",
    "#             for df_type in layer_features.keys():\n",
    "#                 features = layer_features[df_type][subject]\n",
    "#                 labels.append((df_type, len(features)))\n",
    "#                 embeddings += features\n",
    "#             print(f\"Embeddings Built.\")\n",
    "\n",
    "#             tsne(\n",
    "#                 embeddings,\n",
    "#                 labels,\n",
    "#                 perplexities,\n",
    "#                 f\"Layer={target_layer},Subject={subject},Ratio={ratio}\",\n",
    "#                 os.path.join(image_folder, f\"L{target_layer}S{subject}.pdf\")\n",
    "#             )\n",
    "\n",
    "#             print(f\"Subject {subject} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def tsne_decoder(\n",
    "    model_name,\n",
    "    model_config,\n",
    "    num_samples_per_df=50,\n",
    "    perplexities=[20, 50, 80],\n",
    "    save=False,\n",
    "    save_folder=\"./misc/graphs/test/\"\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model, accelerator = get_detector(model_config)\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.Resize(model.encoder.input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(model.encoder.input_resolution),\n",
    "        T.ConvertImageDtype(torch.float32),\n",
    "        T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                    (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "    df_type_list = [\"REAL\", \"DF\", \"NT\", \"FS\", \"F2F\"]\n",
    "    df_features = {df_type: [] for df_type in df_type_list}\n",
    "    for df_type in df_type_list:\n",
    "        c = FFPP.get_default_config()\n",
    "        c.augmentation = \"normal+frame\"\n",
    "        c.compressions = [\"c23\"]\n",
    "        c.types = [df_type]\n",
    "\n",
    "        x = FFPP(c.clone(), 20, 4, transform, accelerator, split=\"test\")\n",
    "        indices = [i for i in range(len(x))]\n",
    "        random.shuffle(indices)\n",
    "        for i in tqdm(indices[:num_samples_per_df]):\n",
    "            data = x[i]\n",
    "            task_logits, other_features = model.predict(\n",
    "                data[0][\"c23\"].unsqueeze(0).to(\"cuda\"),\n",
    "                data[2].unsqueeze(0).to(\"cuda\"),\n",
    "                with_video_features=True\n",
    "            )\n",
    "            df_features[df_type].append(other_features[\"video\"].flatten().tolist())\n",
    "\n",
    "    # with CDF & DFDC\n",
    "    df_features[\"CDF_REAL\"] = []\n",
    "    df_features[\"DFDC_REAL\"] = []\n",
    "    df_features[\"CDF_FAKE\"] = []\n",
    "    df_features[\"DFDC_FAKE\"] = []\n",
    "    for dataset_cls in [CDF, DFDC]:\n",
    "        c = dataset_cls.get_default_config()\n",
    "        x = dataset_cls(c.clone(), 20, 4, transform, accelerator, split=\"test\")\n",
    "        for label in [\"REAL\", \"FAKE\"]:\n",
    "            indices = [i for i in range(len(x))]\n",
    "            indices = [i for i in indices if x.video_info(i)[1] == label]\n",
    "            random.shuffle(indices)\n",
    "            for i in tqdm(indices[:num_samples_per_df]):\n",
    "                data = x[i]\n",
    "                task_logits, other_features = model.predict(\n",
    "                    data[0].unsqueeze(0).to(\"cuda\"),\n",
    "                    data[2].unsqueeze(0).to(\"cuda\"),\n",
    "                    with_video_features=True\n",
    "                )\n",
    "                df_features[f\"{dataset_cls.__name__}_{label}\"].append(other_features[\"video\"].flatten().tolist())\n",
    "\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "    for df_type in df_features.keys():\n",
    "        features = df_features[df_type]\n",
    "        labels.append((df_type, len(features)))\n",
    "        embeddings += features\n",
    "\n",
    "    tsne(\n",
    "        embeddings,\n",
    "        labels,\n",
    "        perplexities,\n",
    "        f\"Categories for '{model_name}'\",\n",
    "        os.path.join(save_folder, f\"{model_name}_category_tsne.pdf\"),\n",
    "        save\n",
    "    )\n",
    "\n",
    "\n",
    "# tsne_decoder(50,perplexities=[20,35,50],detector_path=\"logs/deepfake/test/\")\n",
    "for name, path in [\n",
    "    # (\"BEST\",\"logs/deepfake/deepfake/best/new.yaml\"),\n",
    "    # (\"FARL+tune_all\",\"logs/test/misunderstood-glitter-939/setting.yaml\"),\n",
    "    # (\"JULY(-adapt)+tune_all\", \"logs/test/sage-totem-907/setting.yaml\"),\n",
    "    # (\"JULY+tune_all\", \"logs/test/JULY+tune_all/setting.yaml\")\n",
    "    # (\"LOO(NT)\", \"logs/test/sparkling-firebrand-1108/setting.yaml\"),\n",
    "    (\"2Query\", \"logs/test/silver-gorge-1037/setting.yaml\")\n",
    "]:\n",
    "    tsne_decoder(\n",
    "        name,\n",
    "        path,\n",
    "        50,\n",
    "        perplexities=[20, 35, 50],\n",
    "        save=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9d400f0712968c6b0c2c185442708fcff6ca365f2120d94e1d89a9df5bd30ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
