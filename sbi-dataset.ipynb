{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# initialize accelerator and trackers (if enabled)\n",
                "import copy\n",
                "from src.datasets import FFPP\n",
                "from torch.utils.data import Dataset, default_collate\n",
                "from src.sbi.library.bi_online_generation import random_get_hull\n",
                "from src.sbi.funcs import IoUfrom2bboxes, crop_face, RandomDownScale\n",
                "from src.sbi.initialize import *\n",
                "from src.sbi import blend as B\n",
                "from os import path, scandir, makedirs\n",
                "import albumentations as alb\n",
                "import torchvision\n",
                "import torchvision.transforms as T\n",
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from os import makedirs, path, scandir\n",
                "import pickle\n",
                "import cv2\n",
                "import json\n",
                "from yacs.config import CfgNode as CN\n",
                "from torch.utils.data import Dataset\n",
                "from tqdm import tqdm\n",
                "import logging\n",
                "import random\n",
                "import torch\n",
                "\n",
                "# from src.datasets import FFPP,RPPG\n",
                "from accelerate import Accelerator\n",
                "from main import get_config, init_accelerator, set_seed, FFPP, DFDC, CDF\n",
                "from src.sbi.datasets import SBI\n",
                "logging.basicConfig(level=\"DEBUG\", format='[%(levelname)s][%(filename)s:%(lineno)d]: %(message)s')\n",
                "# TODO:ANCHOR\n",
                "\n",
                "\n",
                "def plot_graphs(imgs, lms=None, text=\"\"):\n",
                "    num_samples = len(imgs)\n",
                "    plt.figure(figsize=(num_samples*4, 4), layout=\"constrained\")\n",
                "    plt.suptitle(text, fontsize=18)\n",
                "    for i in range(len(imgs)):\n",
                "        plt.subplot(1, num_samples, i+1)\n",
                "        plt.imshow(imgs[i])\n",
                "        if not type(lms) == type(None):\n",
                "            plt.scatter(lms[i][:, 0], lms[i][:, 1])\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "class SBI(FFPP):\n",
                "    @staticmethod\n",
                "    def get_default_config():\n",
                "        C = CN()\n",
                "        C.category = 'train'\n",
                "        C.root_dir = './datasets/ffpp/'\n",
                "        C.vid_ext = \".avi\"\n",
                "        C.compressions = ['c23']\n",
                "        C.name = \"SBI\"\n",
                "        C.scale = 1.0\n",
                "        C.pack = False\n",
                "        C.pair = False\n",
                "        C.contrast = False\n",
                "        return C\n",
                "\n",
                "    @staticmethod\n",
                "    def validate_config(config):\n",
                "        config = config.clone()\n",
                "        config.defrost()\n",
                "\n",
                "        assert type(config.category) == str\n",
                "        assert len(config.category) > 0\n",
                "\n",
                "        assert type(config.root_dir) == str\n",
                "        assert len(config.root_dir) > 0\n",
                "\n",
                "        assert type(config.vid_ext) == str\n",
                "        assert len(config.vid_ext) > 0\n",
                "\n",
                "        assert type(config.compressions) == list\n",
                "        assert len(config.compressions) > 0\n",
                "\n",
                "        assert type(config.scale) == float\n",
                "        assert 0 < config.scale <= 1\n",
                "\n",
                "        config.pair = False\n",
                "        config.contrast = True\n",
                "        config.pack = False\n",
                "        config.types = [\"REAL\"]\n",
                "\n",
                "        config.freeze()\n",
                "        return config\n",
                "\n",
                "    def __init__(self, config, num_frames, clip_duration, transform, accelerator, split='train', n_px=224, index=0):\n",
                "        self.TYPE_DIRS = {\n",
                "            'REAL': 'real/'\n",
                "        }\n",
                "        config = self.validate_config(config)\n",
                "\n",
                "        self.category = config.category.lower()\n",
                "        self.name = config.name.lower()\n",
                "        self.root = path.expanduser(config.root_dir)\n",
                "        self.vid_ext = config.vid_ext\n",
                "        self.types = sorted(set(config.types), reverse=True)\n",
                "        self.compressions = sorted(set(config.compressions), reverse=True)\n",
                "        self.num_frames = num_frames\n",
                "        self.clip_duration = clip_duration\n",
                "        self.split = split\n",
                "        self.transform = transform\n",
                "        self.n_px = n_px\n",
                "        self.pack = config.pack\n",
                "        self.pair = config.pair\n",
                "        self.contrast = config.contrast\n",
                "\n",
                "        self.index = index\n",
                "        self.scale = config.scale\n",
                "        # video info list\n",
                "        self.video_list = []\n",
                "\n",
                "        # record missing videos in the csv file for further usage.\n",
                "        self.stray_videos = {}\n",
                "\n",
                "        # stacking data clips\n",
                "        self.stack_video_clips = []\n",
                "\n",
                "        self.transform = transform\n",
                "        self.init_alb_param()\n",
                "        self.general_transoforms = self.get_general_transforms()\n",
                "        self.source_transforms = self.get_source_transforms()\n",
                "\n",
                "        self._build_video_table(accelerator)\n",
                "        self._build_video_list(accelerator)\n",
                "\n",
                "        if split == \"train\":\n",
                "            augmentations = [\n",
                "                alb.RandomResizedCrop(\n",
                "                    self.n_px, self.n_px, scale=(0.8, 1.0), ratio=(1, 1), p=1.0\n",
                "                ),\n",
                "                alb.HorizontalFlip()\n",
                "            ]\n",
                "\n",
                "            self.sequence_augmentation = alb.ReplayCompose(\n",
                "                augmentations,\n",
                "                p=1.\n",
                "            )\n",
                "            self.frame_augmentation = None\n",
                "\n",
                "            def driver(x, replay={}):\n",
                "                # transform to numpy, the alb required format\n",
                "                x = [_x.numpy().transpose((1, 2, 0)) for _x in x]\n",
                "                # frame augmentation\n",
                "                if (not self.frame_augmentation == None):\n",
                "                    if (\"frame\" in replay):\n",
                "                        assert len(replay[\"frame\"]) == len(x), \"Error! frame replay should match the number of frames\"\n",
                "                        x = [\n",
                "                            alb.ReplayCompose.replay(_r, image=_x)[\"image\"] for _x, _r in zip(x, replay[\"frame\"])\n",
                "                        ]\n",
                "\n",
                "                    else:\n",
                "                        replay[\"frame\"] = [None for _ in x]\n",
                "                        for i, _x in enumerate(x):\n",
                "                            result = self.frame_augmentation(image=_x)\n",
                "                            x[i] = result[\"image\"]\n",
                "                            replay[\"frame\"][i] = result[\"replay\"]\n",
                "                # sequence augmentation\n",
                "                if (not self.sequence_augmentation == None):\n",
                "                    if (\"video\" in replay):\n",
                "                        x = [alb.ReplayCompose.replay(replay[\"video\"], image=_x)[\n",
                "                            \"image\"] for _x in x]\n",
                "                    else:\n",
                "                        replay[\"video\"] = self.sequence_augmentation(image=x[0])[\n",
                "                            \"replay\"]\n",
                "                        x = [alb.ReplayCompose.replay(replay[\"video\"], image=_x)[\n",
                "                            \"image\"] for _x in x]\n",
                "                # revert to tensor\n",
                "                x = [torch.from_numpy(_x.transpose((2, 0, 1))) for _x in x]\n",
                "                return x, replay\n",
                "        else:\n",
                "            def driver(x, replay={}):\n",
                "                return x, replay\n",
                "        self.augmentation = driver\n",
                "\n",
                "    def init_alb_param(self):\n",
                "        # Albumentation Strength Parameters\n",
                "        # - Source Transforms\n",
                "        self.src_alb_rgb_bound = (-20, 20)\n",
                "        self.src_alb_rgb_margin = 5\n",
                "        self.src_alb_hue_bound = (-0.3, 0.3)\n",
                "        self.src_alb_hue_margin = 0.1\n",
                "        self.src_alb_bright_bound = (-0.1, 0.1)\n",
                "        self.src_alb_bright_margin = 0.02\n",
                "        self.src_alb_dscale_bound = (1, 3)\n",
                "        self.src_alb_sharpen_params = {\n",
                "            \"alpha\": (0.1, 0.25),\n",
                "            \"lightness\": (0.25, 0.5)\n",
                "        }\n",
                "        # - Affine Transforms\n",
                "        self.aff_alb_aff_params = {\n",
                "            \"translate_percent\": {'x': (-0.05, 0.05), 'y': (-0.01, 0.01)},\n",
                "            \"scale\": [0.8, 1.2],\n",
                "            \"fit_output\": False\n",
                "        }\n",
                "        self.aff_alb_els_params = {\n",
                "            \"alpha\": 10,\n",
                "            \"sigma\": 20,\n",
                "            \"alpha_affine\": 0,\n",
                "        }\n",
                "        # - General Transforms (Train Only)\n",
                "        self.gen_alb_rgb_bound = (-20, 20)\n",
                "        self.gen_alb_hue_bound = (-0.3, 0.3)\n",
                "        self.gen_alb_bright_bound = (-0.3, 0.3)\n",
                "        self.gen_alb_dscale_bound = (1, 1)\n",
                "        self.gen_alb_compress_params = {\n",
                "            \"quality_lower\": 40,\n",
                "            \"quality_upper\": 100,\n",
                "        }\n",
                "        # - Blending Setups\n",
                "        self.blend_random_ratio = (\n",
                "            [0.25, 0.5, 0.75, 1, 1, 1]\n",
                "        )\n",
                "        self.blend_gauss_variance = (\n",
                "            [3.0, 5.0, 7.0, 100, 100]\n",
                "        )\n",
                "\n",
                "    def __len__(self):\n",
                "        return self.stack_video_clips[-1]\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        result = self.get_dict(idx, True)\n",
                "        return *[[_r[name] for _r in result] for name in [\"frames\", \"label\", \"mask\", \"speed\"]], [self.index] * 2\n",
                "\n",
                "    def get_dict(self, idx, block=False):\n",
                "        while (True):\n",
                "            try:\n",
                "                # video_idx =  next(i for i,x in enumerate(self.stack_video_clips) if  idx < x)\n",
                "                # df_type, comp, video_name, clips = self.video_list[video_idx]\n",
                "                video_idx, df_type, comp, video_name, clips = self.video_info(idx)\n",
                "                video_meta = self.video_table[df_type][comp][video_name]\n",
                "                video_offset_duration = (\n",
                "                    idx - (0 if video_idx == 0 else self.stack_video_clips[video_idx - 1])\n",
                "                ) * self.clip_duration\n",
                "                logging.debug(f\"Item/Video Index:{idx}/{video_idx}\")\n",
                "                logging.debug(f\"Item DF/COMP:{df_type}/{comp}\")\n",
                "\n",
                "                if (self.split == \"train\"):\n",
                "                    # the slow motion factor for video data augmentation\n",
                "                    video_speed_factor = random.random() * 0.5 + 0.5\n",
                "                    video_shift_factor = random.random() * (1 - video_speed_factor)\n",
                "                else:\n",
                "                    video_speed_factor = 1\n",
                "                    video_shift_factor = 0\n",
                "\n",
                "                logging.debug(f\"Video Speed Motion Factor: {video_speed_factor}\")\n",
                "                logging.debug(f\"Video Shift Factor: {video_shift_factor}\")\n",
                "\n",
                "                # video frame processing\n",
                "                _frames = []\n",
                "                _landmarks = []\n",
                "\n",
                "                vid_path = video_meta[\"path\"]\n",
                "\n",
                "                vid_reader = torchvision.io.VideoReader(\n",
                "                    vid_path,\n",
                "                    \"video\"\n",
                "                )\n",
                "                # - frames per second\n",
                "                video_sample_freq = vid_reader.get_metadata()[\"video\"][\"fps\"][0]\n",
                "                # - the amount of frames to skip\n",
                "                video_sample_offset = int(\n",
                "                    video_offset_duration + self.clip_duration * video_shift_factor\n",
                "                )\n",
                "                # - the amount of frames for the duration of a clip\n",
                "                video_clip_samples = int(\n",
                "                    video_sample_freq * self.clip_duration * video_speed_factor\n",
                "                )\n",
                "                # - the amount of frames to skip in order to meet the num_frames per clip.(excluding the head & tail frames )\n",
                "                if (self.num_frames == 1):\n",
                "                    video_sample_stride = 0\n",
                "                else:\n",
                "                    video_sample_stride = (\n",
                "                        (video_clip_samples - 1) / (self.num_frames - 1)\n",
                "                    ) / video_sample_freq\n",
                "                logging.debug(f\"Loading Video: {vid_path}\")\n",
                "                logging.debug(f\"Sample Offset: {video_sample_offset}\")\n",
                "                logging.debug(f\"Sample Stride: {video_sample_stride}\")\n",
                "                # load landmark in advance\n",
                "                vid_lm_path = vid_path.replace(\"videos\", \"landmarks\").replace(self.vid_ext, \".npy\")\n",
                "                logging.debug(f\"Loading Landmark: {vid_lm_path}\")\n",
                "                vid_landmarks = np.load(vid_lm_path)\n",
                "                # - fetch frames of clip duration\n",
                "                for sample_idx in range(self.num_frames):\n",
                "                    try:\n",
                "                        _time = video_sample_offset + sample_idx * video_sample_stride\n",
                "                        vid_reader.seek(_time)\n",
                "                        frame = next(vid_reader)\n",
                "                        _frames.append(frame[\"data\"])\n",
                "                        _landmarks.append(vid_landmarks[int(_time*video_sample_freq)])\n",
                "                    except Exception as e:\n",
                "                        raise Exception(f\"unable to read video frame of sample index:{sample_idx}\")\n",
                "                del vid_reader\n",
                "                _frames = [_x.numpy().transpose((1, 2, 0)) for _x in _frames]\n",
                "                plot_graphs(_frames, _landmarks, \"real images\")\n",
                "                _, imgs_f, _, blend_weights, blend_ratio = self.self_blending(\n",
                "                    copy.deepcopy(_frames), copy.deepcopy(_landmarks)\n",
                "                )\n",
                "                logging.debug(f\"blend weights:{blend_weights}\")\n",
                "                logging.debug(f\"blend ratio: {blend_ratio}\")\n",
                "                plot_graphs(imgs_f, _landmarks, \"imgs_f\")\n",
                "                imgs_r = copy.deepcopy(_frames)\n",
                "\n",
                "                # transform for real clip during training:\n",
                "                if (self.split == \"train\"):\n",
                "                    transformed = self.general_transoforms(\n",
                "                        image=imgs_r[0].astype('uint8'),\n",
                "                        **{\n",
                "                            f\"image{i}\": imgs_r[i].astype('uint8')\n",
                "                            for i in range(self.num_frames)\n",
                "                        }\n",
                "                    )\n",
                "                    for i in range(self.num_frames):\n",
                "                        imgs_r[i] = transformed[f'image{i}']\n",
                "\n",
                "                # revert to tensor\n",
                "                imgs_r = [torch.from_numpy(_x.transpose((2, 0, 1))) for _x in imgs_r]\n",
                "                imgs_f = [torch.from_numpy(_x.transpose((2, 0, 1))) for _x in imgs_f]\n",
                "\n",
                "                # perform general size augmentations\n",
                "                imgs_r, replay = self.augmentation(imgs_r, {})\n",
                "                imgs_f, _ = self.augmentation(imgs_f, replay)\n",
                "\n",
                "                # stack list of torch frames to tensor\n",
                "                imgs_r = torch.stack(imgs_r)\n",
                "                imgs_f = torch.stack(imgs_f)\n",
                "\n",
                "                results = []\n",
                "                for clip, label in zip([imgs_r, imgs_f], [0, 1]):\n",
                "                    frames = {}\n",
                "                    # transformation\n",
                "                    if (self.transform):\n",
                "                        clip = self.transform(clip)\n",
                "                    frames[comp] = clip\n",
                "                    logging.debug(f\"Video: SBI {vid_path} for {'FAKE' if label else 'REAL'} , Completed!\")\n",
                "                    # padding and masking missing frames.\n",
                "                    mask = torch.tensor([1.] * len(frames[comp]) +\n",
                "                                        [0.] * (self.num_frames - len(frames[comp])), dtype=torch.bool)\n",
                "\n",
                "                    if len(frames[comp]) < self.num_frames:\n",
                "                        diff = self.num_frames - len(frames[comp])\n",
                "                        padding = torch.zeros((diff, *frames[comp].shape[1:]), dtype=torch.uint8)\n",
                "                        frames[comp] = torch.concatenate((frames[comp], padding))\n",
                "\n",
                "                    results.append({\n",
                "                        \"frames\": frames,\n",
                "                        \"label\": label,\n",
                "                        \"mask\": mask,\n",
                "                        \"speed\": video_speed_factor,\n",
                "                        \"idx\": idx\n",
                "                    })\n",
                "\n",
                "                return results\n",
                "            except Exception as e:\n",
                "                logging.error(f\"Error occur: {e}\")\n",
                "                if block:\n",
                "                    raise e\n",
                "                else:\n",
                "                    idx = random.randrange(0, len(self))\n",
                "\n",
                "    def self_blending(self, imgs, landmarks):\n",
                "        # randomly discard additional face contour coords.\n",
                "        if np.random.rand() < 0.25:\n",
                "            landmarks = [landmarks[i][:68] for i in range(self.num_frames)]\n",
                "        # mask deformation with \"bi\" library.\n",
                "        logging.disable(logging.FATAL)\n",
                "        hull_type = np.random.choice([0, 1, 2, 3])\n",
                "        masks = [\n",
                "            random_get_hull(landmarks[i], imgs[i], hull_type=hull_type)[:, :, 0]\n",
                "            for i in range(self.num_frames)\n",
                "        ]\n",
                "        logging.disable(logging.NOTSET)\n",
                "\n",
                "        sources = copy.deepcopy(imgs)\n",
                "\n",
                "        results = self.source_transforms(\n",
                "            image=imgs[0].astype(np.uint8),\n",
                "            **{\n",
                "                f\"image{i}\": imgs[i].astype(np.uint8) for i in range(self.num_frames)\n",
                "            }\n",
                "        )\n",
                "\n",
                "        _imgs = [results[f\"image{i}\"] for i in range(self.num_frames)]\n",
                "\n",
                "        if np.random.rand() < 0.5:\n",
                "            sources = _imgs\n",
                "        else:\n",
                "            imgs = _imgs\n",
                "        sources, masks = self.randaffine(sources, masks)\n",
                "        plot_graphs(imgs, text=\"target\")\n",
                "        plot_graphs(sources, text=\"sources\")\n",
                "        plot_graphs(masks, text=\"masks\")\n",
                "        ratios = self.blend_random()\n",
                "        logging.debug(f\"Blend Ratios:{ratios}\")\n",
                "        blended_imgs = [None for _ in range(self.num_frames)]\n",
                "        for i in range(self.num_frames):\n",
                "            blended_imgs[i], masks[i] = B.dynamic_blend(sources[i], imgs[i], masks[i], ratio=ratios[i])\n",
                "            blended_imgs[i] = blended_imgs[i].astype(np.uint8)\n",
                "            imgs[i] = imgs[i].astype(np.uint8)\n",
                "        blend_weights = torch.tensor(ratios).float()\n",
                "        blend_weights = (blend_weights / torch.sum(blend_weights, dim=-1)).tolist()\n",
                "        return imgs, blended_imgs, masks, blend_weights, max(ratios)\n",
                "\n",
                "    def blend_mixed(self):\n",
                "        _r = np.random.choice(self.blend_random_ratio)\n",
                "        ratios = [\n",
                "            _r * i for i in self.blend_gaussian()\n",
                "        ]\n",
                "        return ratios\n",
                "\n",
                "    def blend_gaussian(self):\n",
                "        _o = np.random.choice(self.blend_gauss_variance)\n",
                "        _s = np.random.randint(-2, 3)\n",
                "        ratios = [\n",
                "            self.gaussian_pdf((i / (self.num_frames - 1) * 4) - 2 + _s, _o)\n",
                "            for i in range(self.num_frames)\n",
                "        ]\n",
                "        return ratios\n",
                "\n",
                "    def blend_random(self):\n",
                "        _ratio = np.random.choice(self.blend_random_ratio)\n",
                "        ratios = [\n",
                "            _ratio for _ in range(self.num_frames)\n",
                "        ]\n",
                "        return ratios\n",
                "\n",
                "    def blend_hazzard(self):\n",
                "        _ratio = np.random.choice(self.blend_random_ratio)\n",
                "        ratios = [\n",
                "            _ratio * np.random.rand() for _ in range(self.num_frames)\n",
                "        ]\n",
                "        return ratios\n",
                "\n",
                "    def randaffine(self, imgs, masks):\n",
                "        f = alb.Compose(\n",
                "            [\n",
                "                alb.Affine(\n",
                "                    **self.aff_alb_aff_params,\n",
                "                    p=1\n",
                "                )\n",
                "            ],\n",
                "            additional_targets={\n",
                "                **{f'image{i}': 'image' for i in range(0, self.num_frames, 1)},\n",
                "                **{f'mask{i}': 'mask' for i in range(0, self.num_frames, 1)}\n",
                "            },\n",
                "            p=1.\n",
                "        )\n",
                "\n",
                "        g = alb.Compose(\n",
                "            [\n",
                "                alb.ElasticTransform(\n",
                "                    **self.aff_alb_els_params,\n",
                "                    p=1\n",
                "                )\n",
                "            ],\n",
                "            additional_targets={\n",
                "                **{f'image{i}': 'image' for i in range(0, self.num_frames, 1)},\n",
                "                **{f'mask{i}': 'mask' for i in range(0, self.num_frames, 1)}\n",
                "            },\n",
                "            p=1.\n",
                "        )\n",
                "\n",
                "        transformed = f(\n",
                "            image=imgs[0],\n",
                "            **{f\"image{i}\": imgs[i] for i in range(self.num_frames)},\n",
                "            mask=masks[0],\n",
                "            **{f\"mask{i}\": masks[i] for i in range(self.num_frames)},\n",
                "        )\n",
                "\n",
                "        imgs = [transformed[f'image{i}'] for i in range(self.num_frames)]\n",
                "\n",
                "        masks = [transformed[f'mask{i}'] for i in range(self.num_frames)]\n",
                "\n",
                "        transformed = g(\n",
                "            image=imgs[0],\n",
                "            **{f\"image{i}\": imgs[i] for i in range(self.num_frames)},\n",
                "            mask=masks[0],\n",
                "            **{f\"mask{i}\": masks[i] for i in range(self.num_frames)},\n",
                "        )\n",
                "\n",
                "        # only apply elastic deform on mask\n",
                "        masks = [transformed[f'mask{i}'] for i in range(self.num_frames)]\n",
                "\n",
                "        return imgs, masks\n",
                "\n",
                "    def get_general_transforms(self):\n",
                "        return alb.Compose(\n",
                "            [\n",
                "                alb.RGBShift(\n",
                "                    self.gen_alb_rgb_bound,\n",
                "                    self.gen_alb_rgb_bound,\n",
                "                    self.gen_alb_rgb_bound,\n",
                "                    p=0.3\n",
                "                ),\n",
                "                alb.HueSaturationValue(\n",
                "                    self.gen_alb_hue_bound,\n",
                "                    self.gen_alb_hue_bound,\n",
                "                    self.gen_alb_hue_bound,\n",
                "                    p=0.3\n",
                "                ),\n",
                "                alb.RandomBrightnessContrast(\n",
                "                    self.gen_alb_bright_bound,\n",
                "                    self.gen_alb_bright_bound,\n",
                "                    p=0.3\n",
                "                ),\n",
                "                RandomDownScale(\n",
                "                    self.gen_alb_dscale_bound,\n",
                "                    p=0.3\n",
                "                ),\n",
                "                alb.ImageCompression(\n",
                "                    **self.gen_alb_compress_params,\n",
                "                    p=0.5\n",
                "                ),\n",
                "            ],\n",
                "            additional_targets={\n",
                "                f'image{t}_{i}': 'image' for t in range(0, 2) for i in range(0, self.num_frames, 1)\n",
                "            },\n",
                "            p=1.0\n",
                "        )\n",
                "\n",
                "    def get_extend_transforms(self):\n",
                "        return alb.Compose(\n",
                "            alb.OneOf(\n",
                "                [\n",
                "                    alb.GaussNoise(\n",
                "                        var_limit=(100, 200),\n",
                "                        mean=0,\n",
                "                        per_channel=True,\n",
                "                        p=1.0\n",
                "                    ),\n",
                "                    alb.ImageCompression(\n",
                "                        quality_lower=20,\n",
                "                        quality_upper=50,\n",
                "                        p=1.0\n",
                "                    ),\n",
                "                    alb.MultiplicativeNoise(\n",
                "                        multiplier=(0.9, 1.1),\n",
                "                        per_channel=False,\n",
                "                        elementwise=True,\n",
                "                        always_apply=False,\n",
                "                        p=1.0\n",
                "                    ),\n",
                "                ],\n",
                "                p=1.0\n",
                "            ),\n",
                "            additional_targets={\n",
                "                f'image{i}': 'image' for i in range(0, self.num_frames, 1)\n",
                "            },\n",
                "            p=1.0\n",
                "        )\n",
                "\n",
                "    def get_source_transforms(self):\n",
                "        return alb.Compose([\n",
                "            alb.Compose(\n",
                "                [\n",
                "                    # TODO:RESUME\n",
                "                    alb.RGBShift(\n",
                "                        self.src_alb_rgb_bound,\n",
                "                        self.src_alb_rgb_bound,\n",
                "                        self.src_alb_rgb_bound,\n",
                "                        p=1.0\n",
                "                    ),\n",
                "                    alb.HueSaturationValue(\n",
                "                        self.src_alb_hue_bound,\n",
                "                        self.src_alb_hue_bound,\n",
                "                        self.src_alb_hue_bound,\n",
                "                        p=1\n",
                "                    ),\n",
                "                    alb.RandomBrightnessContrast(\n",
                "                        self.src_alb_bright_bound,\n",
                "                        self.src_alb_bright_bound,\n",
                "                        p=1\n",
                "                    ),\n",
                "                ],\n",
                "                p=1\n",
                "            ),\n",
                "            (\n",
                "                alb.OneOf(\n",
                "                    [\n",
                "                        RandomDownScale(self.src_alb_dscale_bound, p=1),\n",
                "                        alb.Sharpen(**self.src_alb_sharpen_params, p=1),\n",
                "                    ],\n",
                "                    p=1\n",
                "                )\n",
                "            ),\n",
                "        ],\n",
                "            additional_targets={\n",
                "                f'image{i}': 'image' for i in range(0, self.num_frames)\n",
                "        },\n",
                "            p=1.\n",
                "        )\n",
                "\n",
                "    def gaussian_pdf(self, x, o):\n",
                "        return pow(e, (-0.5 * pow((x / o), 2)))\n",
                "\n",
                "\n",
                "class Obj:\n",
                "    pass\n",
                "\n",
                "\n",
                "transform = T.Compose([\n",
                "    T.Resize(224, interpolation=T.InterpolationMode.BICUBIC),\n",
                "    T.CenterCrop(224),\n",
                "    T.ConvertImageDtype(torch.float32),\n",
                "])\n",
                "c = SBI.get_default_config()\n",
                "c.compressions = [\"c23\"]\n",
                "c.root_dir = \"./datasets/ffpp/\"\n",
                "c.vid_ext = \".avi\"\n",
                "accelerator = Accelerator(mixed_precision='no')\n",
                "x = SBI(c, 10, 5, transform, accelerator, split=\"train\")\n",
                "x[0]\n",
                "\n",
                "\n",
                "def generate_samples(\n",
                "    sample_num,\n",
                "    num_frames=1,\n",
                "    duration=5,\n",
                "    stride=1,\n",
                "    path=\"./misc/extern/test/sbi_sample/\",\n",
                "    splits=[\"train\", \"val\", \"test\"]\n",
                "):\n",
                "    plt.ioff()\n",
                "    transform = T.Compose([\n",
                "        T.Resize(224, interpolation=T.InterpolationMode.BICUBIC),\n",
                "        T.CenterCrop(224),\n",
                "        T.ConvertImageDtype(torch.float32),\n",
                "    ])\n",
                "    for split in splits:\n",
                "        c = SBI.get_default_config()\n",
                "        c.compressions = [\"c23\"]\n",
                "        c.root_dir = \"./datasets/ffpp/\"\n",
                "        c.vid_ext = \".avi\"\n",
                "        accelerator = Accelerator(mixed_precision='no')\n",
                "        x = SBI(c, num_frames, duration, transform, accelerator, split=split)\n",
                "        folder = os.path.join(path, split)\n",
                "        os.makedirs(folder, exist_ok=True)\n",
                "\n",
                "        for i in range(sample_num):\n",
                "            index = random.randrange(0, len(x))\n",
                "            print(\"INDEX=\", index)\n",
                "            frames, label, mask, _, _ = x[index]\n",
                "            plt.figure(figsize=(2 * (num_frames // stride), 4), layout=\"constrained\")\n",
                "            plt.suptitle(f\"label={label}\")\n",
                "            plt.subplot(2, 1, 1)\n",
                "            plt.title(f\"mask={mask[0]}\")\n",
                "            plt.imshow(\n",
                "                np.stack(\n",
                "                    frames[0][\"c23\"][::stride].numpy().transpose((0, 2, 3, 1)), axis=1\n",
                "                ).reshape((x.n_px, -1, 3))\n",
                "            )\n",
                "            plt.subplot(2, 1, 2)\n",
                "            plt.title(f\"mask={mask[1]}\")\n",
                "            plt.imshow(\n",
                "                np.stack(\n",
                "                    frames[1][\"c23\"][::stride].numpy().transpose((0, 2, 3, 1)), axis=1\n",
                "                ).reshape((x.n_px, -1, 3))\n",
                "            )\n",
                "            # plt.show()\n",
                "            plt.savefig(os.path.join(folder, f\"{i}.jpg\"))\n",
                "            plt.close()\n",
                "            # return"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transform = T.Compose([\n",
                "    T.Resize(224, interpolation=T.InterpolationMode.BICUBIC),\n",
                "    T.CenterCrop(224),\n",
                "    T.ConvertImageDtype(torch.float32),\n",
                "])\n",
                "c = SBI.get_default_config()\n",
                "c.compressions = [\"c23\"]\n",
                "c.root_dir = \"./datasets/ffpp/\"\n",
                "c.vid_ext = \".avi\"\n",
                "accelerator = Accelerator(mixed_precision='no')\n",
                "x = SBI(c, 10, 5, transform, accelerator, split=\"train\")\n",
                "c"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# x.video_table[\"REAL\"][\"c23\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# typ, cmp, idx, _ = x.video_list[1234]\n",
                "# x.video_table[typ][cmp][idx]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# a = x.get_dict(7079, True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "frames, label, mask, _, _ = x[random.randint(0, len(x))]\n",
                "# frames, label, mask, _, _ = x[800]\n",
                "# frames, label, mask, _, _ = x[9751]\n",
                "(len(frames), label, len(mask))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "frames[0].keys(), frames[1].keys()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "plt.figure(figsize=(50, 5))\n",
                "plt.subplot(2, 1, 1)\n",
                "plt.imshow(\n",
                "    np.stack(\n",
                "        frames[0][\"c23\"][:30].numpy().transpose((0, 2, 3, 1)), axis=1\n",
                "    ).reshape((x.n_px, -1, 3))\n",
                ")\n",
                "plt.subplot(2, 1, 2)\n",
                "plt.imshow(\n",
                "    np.stack(\n",
                "        frames[1][\"c23\"][:30].numpy().transpose((0, 2, 3, 1)), axis=1\n",
                "    ).reshape((x.n_px, -1, 3))\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# for i in tqdm(range(len(x))):\n",
                "#     try:\n",
                "#         x[i]\n",
                "#     except Exception as e:\n",
                "#         print(f\"Error Occur at {i}:{e}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "dfd-clip",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.16"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "c9d400f0712968c6b0c2c185442708fcff6ca365f2120d94e1d89a9df5bd30ab"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
