{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import logging\n",
    "import statistics\n",
    "import src.clip as clip\n",
    "from torchinfo import summary\n",
    "import torchvision.transforms as T\n",
    "from accelerate import Accelerator\n",
    "from yacs.config import CfgNode as CN\n",
    "from main import get_config, init_accelerator, set_seed, FFPP\n",
    "from src.models import Detector\n",
    "logging.basicConfig(level=\"DEBUG\")\n",
    "\n",
    "\n",
    "class Obj:\n",
    "    pass\n",
    "\n",
    "\n",
    "c = FFPP.get_default_config()\n",
    "c.augmentation = \"normal+frame\"\n",
    "c.pair = 1\n",
    "c.compressions = [\"c23\"]\n",
    "c.types = [\"REAL\", \"NT\"]\n",
    "\n",
    "\n",
    "mc = Detector.get_default_config()\n",
    "mc.out_dim = [2]\n",
    "mc.adapter = CN()\n",
    "mc.adapter.frozen = 0\n",
    "mc.adapter.struct = CN()\n",
    "# mc.adapter.struct.type = \"legacy-768-x-768\"\n",
    "mc.adapter.struct.type = \"768-x-768-nln\"\n",
    "mc.adapter.struct.x = 256\n",
    "mc.adapter.type = \"normal\"\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='no')\n",
    "model = Detector(mc, 20, accelerator).to(accelerator.device).eval()\n",
    "# encoder = clip.load(\"ViT-B/16\")[0].visual.float()\n",
    "encoder = model.encoder\n",
    "adapter = model.adapter\n",
    "adapter.load_state_dict(\n",
    "    {\n",
    "        k[8:]: v for k, v in\n",
    "        torch.load(\n",
    "            # \"/home/od/Desktop/repos/dfd-clip/logs/comp-inv/comp-inv/mode1+256+resi+1e-2/last_weights.pt\",\n",
    "            \"/home/od/Desktop/repos/dfd-clip/logs/test/0520T1305/best_weights.pt\",\n",
    "            map_location=\"cpu\"\n",
    "        ).items()\n",
    "        if \"adapter\" in k\n",
    "    }\n",
    ")\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(encoder.input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(encoder.input_resolution),\n",
    "    T.ConvertImageDtype(torch.float32),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "x = FFPP(c.clone(), 20, 5, transform, accelerator, split=\"train\")\n",
    "_x = FFPP(c.clone(), 20, 5, lambda x: x, accelerator, split=\"train\")\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.randint(0, len(x))\n",
    "data = x[idx]\n",
    "_data = _x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(data[0][\"raw\"][0].numpy().transpose((1, 2, 0)))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(data[0][\"c23\"][0].numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = {}\n",
    "# nfeatures = {}\n",
    "# with torch.no_grad():\n",
    "#     for k, v in data[0].items():\n",
    "#         # get key and value from each CLIP ViT layer\n",
    "#         kvs = encoder(v[0].unsqueeze(0).to(\"cuda\"))\n",
    "#         # discard original CLS token and restore temporal dimension\n",
    "#         kvs = [{k: v[:, 1:] for k, v in kv.items()} for kv in kvs]\n",
    "\n",
    "#         _kvs = [{k: v.unsqueeze(0) for k, v in kv.items()} for kv in kvs]\n",
    "#         _kvs = adapter([kvs[i] for i in range(0, 12, 2)])\n",
    "\n",
    "#         kvs = [{k: v.squeeze(0).view((-1, 768)).to(\"cpu\") for k, v in kv.items()} for kv in kvs]\n",
    "#         features[k] = kvs\n",
    "#         _kvs = [{k: v.squeeze(0).view((-1, 768)).to(\"cpu\") for k, v in kv.items()} for kv in _kvs]\n",
    "#         nfeatures[k] = _kvs\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "nfeatures = {}\n",
    "with torch.no_grad():\n",
    "    for k, v in data[0].items():\n",
    "        # get key and value from each CLIP ViT layer\n",
    "        kvs = encoder(v.to(\"cuda\"))\n",
    "        # discard original CLS token and restore temporal dimension\n",
    "        kvs = [{k: v[:, 1:] for k, v in kv.items()} for kv in kvs]\n",
    "\n",
    "        _kvs = [{k: v.unsqueeze(0) for k, v in kv.items()} for kv in kvs]\n",
    "        _kvs = adapter([_kvs[i] for i in [5, 6, 7, 8, 9, 10]])\n",
    "        kvs = [{k: v[0].view((-1, 768)).to(\"cpu\") for k, v in kv.items()} for kv in kvs]\n",
    "        features[k] = kvs\n",
    "        _kvs = [{k: v[:, 0].view((-1, 768)).to(\"cpu\") for k, v in kv.items()} for kv in _kvs]\n",
    "        nfeatures[k] = _kvs\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = {\n",
    "    \"k\": [],\n",
    "    \"v\": []\n",
    "}\n",
    "for layer in range(12):\n",
    "    for subject in [\"k\", \"v\"]:\n",
    "        l[subject].append(\n",
    "            torch.nn.functional.mse_loss(\n",
    "                features[\"raw\"][layer][subject],\n",
    "                features[\"c23\"][layer][subject],\n",
    "                reduction=\"none\"\n",
    "            ).mean(dim=-1).view(14, 14).numpy()\n",
    "        )\n",
    "methods.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = {\n",
    "    \"k\": [],\n",
    "    \"v\": []\n",
    "}\n",
    "for layer in range(12):\n",
    "    for subject in [\"k\", \"v\"]:\n",
    "        l[subject].append(\n",
    "            torch.nn.functional.kl_div(\n",
    "                torch.nn.functional.log_softmax(features[\"raw\"][layer][subject], dim=-1),\n",
    "                torch.nn.functional.log_softmax(features[\"c23\"][layer][subject], dim=-1),\n",
    "                log_target=True,\n",
    "                reduction=\"none\"\n",
    "            ).mean(dim=-1).view(14, 14).numpy()\n",
    "        )\n",
    "methods.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "for l in methods:\n",
    "    plt.figure(figsize=(36, 6))\n",
    "    for j, s in enumerate([\"k\", \"v\"]):\n",
    "        for i, v in enumerate(l[s]):\n",
    "            plt.subplot(2, 12, j * 12 + i + 1)\n",
    "            im = plt.imshow(v)\n",
    "\n",
    "            # Show all ticks and label them with the respective list entries\n",
    "            plt.gca().set_xticks(np.arange(14))\n",
    "            plt.gca().set_yticks(np.arange(14))\n",
    "\n",
    "            # Rotate the tick labels and set their alignment.\n",
    "            # plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            #          rotation_mode=\"anchor\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmethods = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = {\n",
    "    \"k\": [],\n",
    "    \"v\": []\n",
    "}\n",
    "for layer in range(6):\n",
    "    for subject in [\"k\", \"v\"]:\n",
    "        l[subject].append(\n",
    "            torch.nn.functional.mse_loss(\n",
    "                nfeatures[\"raw\"][layer][subject],\n",
    "                nfeatures[\"c23\"][layer][subject],\n",
    "                reduction=\"none\"\n",
    "            ).mean(dim=-1).view(14, 14).numpy()\n",
    "        )\n",
    "nmethods.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = {\n",
    "    \"k\": [],\n",
    "    \"v\": []\n",
    "}\n",
    "for layer in range(6):\n",
    "    for subject in [\"k\", \"v\"]:\n",
    "        l[subject].append(\n",
    "            torch.nn.functional.kl_div(\n",
    "                torch.nn.functional.log_softmax(nfeatures[\"raw\"][layer][subject], dim=-1),\n",
    "                torch.nn.functional.log_softmax(nfeatures[\"c23\"][layer][subject], dim=-1),\n",
    "                log_target=True,\n",
    "                reduction=\"none\"\n",
    "            ).mean(dim=-1).view(14, 14).numpy()\n",
    "        )\n",
    "nmethods.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "loc = [5, 6, 7, 8, 9, 10]\n",
    "for l in nmethods:\n",
    "    plt.figure(figsize=(36, 6))\n",
    "    for j, s in enumerate([\"k\", \"v\"]):\n",
    "        for i, v in enumerate(l[s]):\n",
    "            plt.subplot(2, 12, j * 12 + loc[i] + 1)\n",
    "            im = plt.imshow(v)\n",
    "\n",
    "            # Show all ticks and label them with the respective list entries\n",
    "            plt.gca().set_xticks(np.arange(14))\n",
    "            plt.gca().set_yticks(np.arange(14))\n",
    "\n",
    "            # Rotate the tick labels and set their alignment.\n",
    "            # plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            #          rotation_mode=\"anchor\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nmethods[0][\"k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9d400f0712968c6b0c2c185442708fcff6ca365f2120d94e1d89a9df5bd30ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
