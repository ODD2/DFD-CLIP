{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import statistics\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from accelerate import Accelerator\n",
    "from yacs.config import CfgNode as CN\n",
    "from main import get_config, init_accelerator, set_seed, FFPP\n",
    "from src.models import Detector, remap_weight\n",
    "logging.basicConfig(level=\"DEBUG\")\n",
    "\n",
    "\n",
    "class Obj:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"logs/vlvpt/VLVPT/splendid-cherry-155/setting.yaml\"  # STA\n",
    "# cfg_path = \"logs/vlvpt/VLVPT/mild-sunset-151/setting.yaml\"  # +SA\n",
    "cfg_path = \"logs/vlvpt/VLVPT/olive-sponge-143/setting.yaml\"  # +guide\n",
    "# cfg_path = \"logs/vlvpt/VLVPT/devout-aardvark-138/setting.yaml\"  # all\n",
    "cfg_path = \"logs/vlvpt/VLVPT/swept-dragon-194/setting.yaml\"\n",
    "\n",
    "with open(cfg_path) as f:\n",
    "    preset = CN(yaml.safe_load(f))\n",
    "\n",
    "num_frames = preset.data.num_frames\n",
    "mc = Detector.get_default_config().merge_from_other_cfg(preset.model)\n",
    "mc.op_mode.attn_record = True\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='no')\n",
    "\n",
    "model = Detector(mc, num_frames, accelerator).to(accelerator.device)\n",
    "encoder = model.encoder\n",
    "model.load_state_dict(\n",
    "    remap_weight(\n",
    "        torch.load(\n",
    "            os.path.join(os.path.split(cfg_path)[0], \"best_weights.pt\"),\n",
    "            map_location=\"cpu\"\n",
    "        ),\n",
    "        model\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = FFPP.get_default_config()\n",
    "c.pack = True\n",
    "c.augmentation = \"normal+rrc\"\n",
    "c.random_speed = False\n",
    "c.compressions = [\"c23\"]\n",
    "# c.types = [\"REAL\", \"FS\",\"F2F\",\"DF\"]\n",
    "c.types = [\"REAL\", \"NT\", \"FS\", \"F2F\", \"DF\"]\n",
    "# c.types = [\"FS\"]\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(encoder.input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(encoder.input_resolution),\n",
    "    T.ConvertImageDtype(torch.float32),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "_transform = T.Compose([\n",
    "    T.Resize(encoder.input_resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(encoder.input_resolution),\n",
    "    T.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "# x = FFPP(c.clone(), num_frames, 4, transform, accelerator, split=\"test\")\n",
    "# _x = FFPP(c.clone(), num_frames, 4, _transform, accelerator, split=\"test\")\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def interpret(clip, mask, model, device, start_layer=0, num_layer=-1, logit_index=1, mode=\"all\", combine_query=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    # duplicate image for further gradient operation to accord to different input sources\n",
    "    num_patch = model.encoder.input_resolution // model.encoder.patch_size\n",
    "    task_logits, features = model.predict(clip.unsqueeze(0).to(device), mask.unsqueeze(0).to(device))\n",
    "    clip_logits = task_logits[0]\n",
    "    probs = clip_logits.softmax(dim=-1).detach().cpu().numpy()\n",
    "    print(\"Score:\", clip_logits)\n",
    "    print(\"Probs:\", probs)\n",
    "    one_hot = np.zeros((1, 1), dtype=np.float32)\n",
    "    # diagonal matrix\n",
    "    one_hot[0, 0] = 1\n",
    "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "    # only observe the fake logit score\n",
    "    one_hot = torch.sum(one_hot.cuda() * clip_logits[:, logit_index])\n",
    "    model.zero_grad()\n",
    "\n",
    "    image_attn_blocks = list(dict(model.decoder.transformer.resblocks.named_children()).values())\n",
    "\n",
    "    if start_layer == -1:\n",
    "        # calculate index of last layer\n",
    "        start_layer = len(image_attn_blocks) - 1\n",
    "\n",
    "    if num_layer == -1:\n",
    "        num_layer = len(image_attn_blocks)\n",
    "\n",
    "    num_tokens = image_attn_blocks[0].attn.aff.shape[-2]\n",
    "    num_queries = image_attn_blocks[0].attn.aff.shape[1]\n",
    "    # create empty storage\n",
    "    R = torch.zeros(num_queries, num_tokens, dtype=image_attn_blocks[0].attn.aff.dtype).to(device)\n",
    "    # create saliency map for each input-output in interest\n",
    "    R = R.unsqueeze(0)\n",
    "    for i in range(start_layer, start_layer + num_layer):\n",
    "        # calculate the gradient of attn_prob wrt the one_hot score(each pair of text and image)\n",
    "        # the blk.attn.aff has shape of (12*n, 50, 50), n = batch_size, which is number of texts in this context.\n",
    "        # 50 = 7*7 + 1, which 7*7 is the number of patches and 1 for the cls token.\n",
    "        # The grad function will derive the gradient of the attention scores over the heads of the n image corresponding to the texts.\n",
    "        # In this step, the purpose of duplicating the image according to the number of texts and creating the one_hot diagonal matrix\n",
    "        # for the similarity score supported the purpose of splitting the affect of the image features corresponding to different texts.\n",
    "        grad = torch.autograd.grad(\n",
    "            one_hot,\n",
    "            [image_attn_blocks[i].attn.aff],\n",
    "            retain_graph=True\n",
    "        )[0].detach()\n",
    "        grad = torch.nan_to_num(grad, 0)\n",
    "        cam = image_attn_blocks[i].attn.aff.detach()\n",
    "\n",
    "        # cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        # grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "\n",
    "        # align the gradient with the cam score according to the elements positivity\n",
    "        # (CODA could have negative cam values, but that doesn't mean it has low attention score,\n",
    "        # the value it self should infact indicate the importance rather the sign.\n",
    "        # If an element has both negative cam and grad values, this should indicate positive impact,\n",
    "        # just like both having positive values.)\n",
    "        grad = grad * (cam / torch.abs(cam))\n",
    "        if (mode == \"grad\"):\n",
    "            cam = grad\n",
    "        elif (mode == \"cam\"):\n",
    "            cam = cam\n",
    "        elif (mode == \"all\"):\n",
    "            # due to coda, the value of the element indicates the importance, therefore abs is required.\n",
    "            cam = torch.abs(cam)\n",
    "\n",
    "            # regularize the overall representation, these causes bad visualizations.\n",
    "            # norm\n",
    "            # cam = (cam - cam.min())/(cam.max()-cam.min())\n",
    "            # softmax\n",
    "            # cam = cam.softmax(dim=-2)\n",
    "            cam = grad * cam\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        # reshape to restore (n,h,p^2+1,p^2+1)\n",
    "        # cam = cam.reshape(1, -1, cam.shape[-1], cam.shape[-1])\n",
    "        cam = cam.permute((0, 3, 1, 2))\n",
    "\n",
    "        # average over each heads.\n",
    "        cam = cam.clamp(min=0).mean(dim=1)\n",
    "        # the cam represents the multi-head averaged gradient weighted attention score for each frame corresponding to the texts.\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        R = R + cam / num_layer\n",
    "\n",
    "    # video_relevance = R[:, 0]\n",
    "    if combine_query:\n",
    "        video_relevance = torch.sum(R, dim=1)\n",
    "        video_relevance = video_relevance.view((1, -1, num_patch, num_patch))\n",
    "    else:\n",
    "        video_relevance = R\n",
    "        video_relevance = video_relevance.view((num_queries, -1, num_patch, num_patch))\n",
    "    return video_relevance, probs\n",
    "\n",
    "\n",
    "def show_clip_relevance(clip, clip_relevance, q=0):\n",
    "    clip_relevance = clip_relevance[q]\n",
    "    # create heatmap from mask on image\n",
    "\n",
    "    def show_cam_on_clip(clip, mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "        cam = heatmap + np.float32(clip)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    dim = int(clip_relevance.shape[-1])\n",
    "    # process clip relevance map\n",
    "    clip_relevance = clip_relevance.unsqueeze(1)\n",
    "    clip_relevance = torch.nn.functional.interpolate(clip_relevance, size=224, mode='bilinear').cpu().numpy()\n",
    "    clip_relevance = (clip_relevance - clip_relevance.min()) / (clip_relevance.max() - clip_relevance.min())\n",
    "    # process clip frames\n",
    "    clip = (clip - clip.min()) / (clip.max() - clip.min())\n",
    "    clip = clip.cpu().numpy()\n",
    "\n",
    "    # convert frame/cam sequence into a large frame\n",
    "    clip_relevance = clip_relevance.transpose(2, 0, 3, 1).reshape(224, -1, 1)\n",
    "    clip = clip.transpose(2, 0, 3, 1).reshape(224, -1, 3)\n",
    "    # process\n",
    "    cam = show_cam_on_clip(clip, clip_relevance)\n",
    "    cam = np.uint8(255 * cam)\n",
    "    cam = cv2.cvtColor(np.array(cam), cv2.COLOR_RGB2BGR)\n",
    "    clip = np.uint8(255 * clip)\n",
    "    return clip, cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "clip_heatmap_pairs = []\n",
    "# Cs = list(range(model.decoder.num_classes))\n",
    "# Ds = list([range(1,len(model.layer_indices+1)))\n",
    "# Qs = list(range(model.decoder.num_classes))\n",
    "Cs = [1]\n",
    "Ds = [6]\n",
    "Qs = [0, 1]\n",
    "\n",
    "# for dftype in [\"NT\",\"DF\",\"FS\"]:\n",
    "# for dftype in [\"REAL\", \"DF\", \"NT\"]:\n",
    "# for dftype in [\"REAL\", \"NT\", \"FS\", \"F2F\", \"DF\"]:\n",
    "for dftype in [\"NT\"]:\n",
    "    c.types = [dftype]\n",
    "    x = FFPP(c.clone(), num_frames, 4, transform, accelerator, split=\"test\")\n",
    "    # _x = FFPP(c.clone(), num_frames, 4, _transform, accelerator, split=\"train\")\n",
    "\n",
    "    # select video index\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if d[2] == \"691_732\")\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if d[2] == \"851_552\")\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if d[2] == \"691_732\")\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if d[2] == \"089\")\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if \"225\" == d[2].split('_')[0])\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if \"634\" in d[2])\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if d[2] == \"507_418\")\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if d[0]==\"DF\" and d[2] == \"855_801\")\n",
    "    # vid_idx = next(i for i, d in enumerate(x.video_list) if d[2] == \"855\")\n",
    "    # vid_idx = 10\n",
    "    vid_idx = random.randrange(0, len(x))\n",
    "\n",
    "    # sample video\n",
    "    clips, label, masks, speed, meta, task_index = x[vid_idx]\n",
    "    _clips = x[vid_idx][0]\n",
    "    clips = torch.stack(clips)\n",
    "    masks = torch.stack(masks)\n",
    "\n",
    "    # select single clip\n",
    "    clip = clips[0]\n",
    "    mask = masks[0]\n",
    "    _clip = _clips[0]\n",
    "\n",
    "    # generate\n",
    "    for logit in Cs:\n",
    "        for depth in Ds:\n",
    "            # interpret prediction\n",
    "            _clip_relevance, probs = interpret(\n",
    "                clip, mask, model, \"cuda\", logit_index=logit,\n",
    "                start_layer=0, num_layer=depth, mode=\"all\"\n",
    "            )\n",
    "            probs = [round(p, 3) for p in probs.flatten().tolist()]\n",
    "            # heatmap for individual queries\n",
    "            for q in Qs:\n",
    "                flat_clip, flat_heatmap = show_clip_relevance(_clip, _clip_relevance, q)\n",
    "                clip_heatmap_pairs.append(\n",
    "                    (f\"{dftype},Logit:{logit},Prob:{probs},Q:{q},Depth:{depth}\", flat_clip, flat_heatmap)\n",
    "                )\n",
    "\n",
    "\n",
    "# plot figures\n",
    "num_figures = 2 * len(clip_heatmap_pairs)\n",
    "plt.figure(figsize=(1.5 * num_frames, 1.5 * num_figures), layout=\"constrained\")\n",
    "for i, data in enumerate(clip_heatmap_pairs):\n",
    "    title, flat_clip, flat_heatmap = data\n",
    "    plt.subplot(num_figures, 1, i * 2 + 1)\n",
    "    plt.title(title)\n",
    "    plt.imshow(flat_clip)\n",
    "    plt.gca().axis(\"off\")\n",
    "    plt.subplot(num_figures, 1, i * 2 + 2)\n",
    "    plt.imshow(flat_heatmap)\n",
    "    plt.gca().axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
