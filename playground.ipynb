{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/od/Desktop/Dataset/CelebDF/Real/videos/id12_0000.mp4',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/videos/00266.mp4',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/videos/id39_0005.mp4',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/videos/00258.mp4',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/videos/id10_0007.mp4',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/videos/id1_0001.mp4',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/videos/00061.mp4',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/videos/id22_0000.mp4']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "with open(\"./pickles/cdf_real2.pickle\", \"rb\") as f:\n",
    "    vlist = pickle.load(f)\n",
    "vlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces/id12_0000.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces_250/id12_0000.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/landmarks/id12_0000.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id12_0000_notate.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id12_0000.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces(landmark)/id12_0000.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces/00266.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces_250/00266.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/landmarks/00266.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/00266_notate.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/00266.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces(landmark)/00266.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces/id39_0005.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces_250/id39_0005.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/landmarks/id39_0005.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id39_0005_notate.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id39_0005.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces(landmark)/id39_0005.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces/00258.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces_250/00258.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/landmarks/00258.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/00258.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/00258_notate.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces(landmark)/00258.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces/id10_0007.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces_250/id10_0007.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/landmarks/id10_0007.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id10_0007_notate.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id10_0007.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces(landmark)/id10_0007.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces/id1_0001.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces_250/id1_0001.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/landmarks/id1_0001.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id1_0001.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id1_0001_notate.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces(landmark)/id1_0001.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces/00061.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces_250/00061.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/landmarks/00061.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/00061.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/00061_notate.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces(landmark)/00061.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces/id22_0000.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces_250/id22_0000.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/landmarks/id22_0000.npy',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id22_0000_notate.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/semantic_videos/id22_0000.avi',\n",
       " '/home/od/Desktop/Dataset/CelebDF/Real/cropped_faces(landmark)/id22_0000.npy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "folders = [\"cropped_faces\", \"cropped_faces_250\", \"landmarks\", \"semantic_videos\", \"cropped_faces(landmark)\"]\n",
    "files = []\n",
    "for v in vlist:\n",
    "    for folder in folders:\n",
    "        # files.extend(glob(f\"/home/od/Desktop/Dataset/DFDC/{folder}/{os.path.split(v)[-1][:-4]}*.*\"))\n",
    "      files.extend(glob(v.replace(\"videos\",folder).replace(\".mp4\",\"*.*\")))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files),len(vlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "\n",
    "# video_path = \"/home/od/Desktop/Dataset/CelebDF/Fake/cropped_faces/id0_id1_0000.avi\"\n",
    "video_path = \"/home/od/Desktop/Dataset/DFDC/videos/nwpjsjarju.mp4\"\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_indices = [i for i in range(0, 300, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.load(f\"/home/od/Desktop/Dataset/DFDC/test_lm/rfwtnztcsj.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.reshape(-1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# i = random.randrange(0, len(frame_indices))\n",
    "# print(i)\n",
    "# plt.figure(figsize=(25, 15))\n",
    "# plt.imshow(cv2.cvtColor(frames[frame_indices[i]], cv2.COLOR_BGR2RGB))\n",
    "# plt.scatter(landmarks[i, :, 0], landmarks[i, :, 1], alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "\n",
    "# video_path = \"/home/od/Desktop/Dataset/CelebDF/Fake/cropped_faces/id0_id1_0000.avi\"\n",
    "video_path = \"/stock/FaceForensicC23/videos/NT/217_117.mp4\"\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceData:\n",
    "    def __init__(self, _lm, _bbox, _idx):\n",
    "        self.ema_lm = _lm\n",
    "        self.lm = [_lm]\n",
    "        self.bbox = [_bbox]\n",
    "        self.idx = [_idx]\n",
    "\n",
    "    def last(self):\n",
    "        return self.ema_lm\n",
    "\n",
    "    def add(self, _lm, _bbox, _idx):\n",
    "        self.ema_lm = self.ema_lm*0.5 +_lm * 0.5\n",
    "        self.lm.append(_lm)\n",
    "        self.bbox.append(_bbox)\n",
    "        self.idx.append(_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./pickles/217_117.pickle\",\"rb\") as f:\n",
    "    frame_faces = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dbs = []\n",
    "for faces, index in zip(frame_faces, frame_indices):\n",
    "    frame_landmarks = np.stack([face[\"landmarks\"] for face in faces])\n",
    "    matched_indices = []\n",
    "\n",
    "    for face_data in face_dbs:\n",
    "\n",
    "        lm_diff = np.sum(\n",
    "            np.linalg.norm(frame_landmarks - face_data.last(), axis=-1),\n",
    "            axis=1\n",
    "        ) / frame_landmarks.shape[1]\n",
    "\n",
    "        if (np.min(lm_diff) > 100):\n",
    "            continue\n",
    "\n",
    "        closest_idx = np.argmin(lm_diff)\n",
    "        matched_indices.append(closest_idx)\n",
    "        face_data.add(faces[closest_idx][\"landmarks\"], faces[closest_idx][\"bbox\"], index)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        if i in matched_indices:\n",
    "            continue\n",
    "        else:\n",
    "            _landmark = face['landmarks']\n",
    "            _bbox = face['bbox']\n",
    "            face_dbs.append(FaceData(_landmark, _bbox, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(face_dbs),len(face_dbs[0].lm),len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted(face_dbs,key=lambda x: len(x),reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(0,len(frames)) if not i in face_dbs[0].idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "fid = 0\n",
    "i = random.randrange(0, len(face_dbs[fid]))\n",
    "i =210\n",
    "landmarks = face_dbs[fid].lm[i]\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.imshow(cv2.cvtColor(frames[face_dbs[fid].idx[i]], cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], alpha=0.5, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = 211\n",
    "faces = [face[\"landmarks\"] for face in frame_faces[idx]]\n",
    "print(len(faces))\n",
    "landmarks =np.stack(faces).reshape((-1,2))\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.imshow(cv2.cvtColor(frames[idx], cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], alpha=0.5, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
