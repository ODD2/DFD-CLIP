{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, IntEnum\n",
    "\n",
    "\n",
    "class T(IntEnum):\n",
    "    A = 0\n",
    "\n",
    "\n",
    "type(T.A)\n",
    "int(T.A)\n",
    "T.A == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "with open(\"./pickles/cdf_real2.pickle\", \"rb\") as f:\n",
    "    vlist = pickle.load(f)\n",
    "vlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "folders = [\"cropped_faces\", \"cropped_faces_250\", \"landmarks\", \"semantic_videos\", \"cropped_faces(landmark)\"]\n",
    "files = []\n",
    "for v in vlist:\n",
    "    for folder in folders:\n",
    "        # files.extend(glob(f\"/home/od/Desktop/Dataset/DFDC/{folder}/{os.path.split(v)[-1][:-4]}*.*\"))\n",
    "        files.extend(glob(v.replace(\"videos\", folder).replace(\".mp4\", \"*.*\")))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files), len(vlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "\n",
    "# video_path = \"/home/od/Desktop/Dataset/CelebDF/Fake/cropped_faces/id0_id1_0000.avi\"\n",
    "video_path = \"/home/od/Desktop/Dataset/DFDC/videos/nwpjsjarju.mp4\"\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_indices = [i for i in range(0, 300, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.load(f\"/home/od/Desktop/Dataset/DFDC/test_lm/rfwtnztcsj.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.reshape(-1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# i = random.randrange(0, len(frame_indices))\n",
    "# print(i)\n",
    "# plt.figure(figsize=(25, 15))\n",
    "# plt.imshow(cv2.cvtColor(frames[frame_indices[i]], cv2.COLOR_BGR2RGB))\n",
    "# plt.scatter(landmarks[i, :, 0], landmarks[i, :, 1], alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "\n",
    "# video_path = \"/home/od/Desktop/Dataset/CelebDF/Fake/cropped_faces/id0_id1_0000.avi\"\n",
    "video_path = \"/stock/FaceForensicC23/videos/NT/217_117.mp4\"\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceData:\n",
    "    def __init__(self, _lm, _bbox, _idx):\n",
    "        self.ema_lm = _lm\n",
    "        self.lm = [_lm]\n",
    "        self.bbox = [_bbox]\n",
    "        self.idx = [_idx]\n",
    "\n",
    "    def last(self):\n",
    "        return self.ema_lm\n",
    "\n",
    "    def add(self, _lm, _bbox, _idx):\n",
    "        self.ema_lm = self.ema_lm * 0.5 + _lm * 0.5\n",
    "        self.lm.append(_lm)\n",
    "        self.bbox.append(_bbox)\n",
    "        self.idx.append(_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./pickles/217_117.pickle\", \"rb\") as f:\n",
    "    frame_faces = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dbs = []\n",
    "for faces, index in zip(frame_faces, frame_indices):\n",
    "    frame_landmarks = np.stack([face[\"landmarks\"] for face in faces])\n",
    "    matched_indices = []\n",
    "\n",
    "    for face_data in face_dbs:\n",
    "\n",
    "        lm_diff = np.sum(\n",
    "            np.linalg.norm(frame_landmarks - face_data.last(), axis=-1),\n",
    "            axis=1\n",
    "        ) / frame_landmarks.shape[1]\n",
    "\n",
    "        if (np.min(lm_diff) > 100):\n",
    "            continue\n",
    "\n",
    "        closest_idx = np.argmin(lm_diff)\n",
    "        matched_indices.append(closest_idx)\n",
    "        face_data.add(faces[closest_idx][\"landmarks\"], faces[closest_idx][\"bbox\"], index)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        if i in matched_indices:\n",
    "            continue\n",
    "        else:\n",
    "            _landmark = face['landmarks']\n",
    "            _bbox = face['bbox']\n",
    "            face_dbs.append(FaceData(_landmark, _bbox, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(face_dbs), len(face_dbs[0].lm), len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted(face_dbs, key=lambda x: len(x), reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(0, len(frames)) if not i in face_dbs[0].idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "fid = 0\n",
    "i = random.randrange(0, len(face_dbs[fid]))\n",
    "i = 210\n",
    "landmarks = face_dbs[fid].lm[i]\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.imshow(cv2.cvtColor(frames[face_dbs[fid].idx[i]], cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], alpha=0.5, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = 211\n",
    "faces = [face[\"landmarks\"] for face in frame_faces[idx]]\n",
    "print(len(faces))\n",
    "landmarks = np.stack(faces).reshape((-1, 2))\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.imshow(cv2.cvtColor(frames[idx], cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], alpha=0.5, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['visual.mask_token', 'visual.lm_transformer.resblocks.0.attn.in_proj_weight', 'visual.lm_transformer.resblocks.0.attn.in_proj_bias', 'visual.lm_transformer.resblocks.0.attn.out_proj.weight', 'visual.lm_transformer.resblocks.0.attn.out_proj.bias', 'visual.lm_transformer.resblocks.0.ln_1.weight', 'visual.lm_transformer.resblocks.0.ln_1.bias', 'visual.lm_transformer.resblocks.0.mlp.c_fc.weight', 'visual.lm_transformer.resblocks.0.mlp.c_fc.bias', 'visual.lm_transformer.resblocks.0.mlp.c_proj.weight', 'visual.lm_transformer.resblocks.0.mlp.c_proj.bias', 'visual.lm_transformer.resblocks.0.ln_2.weight', 'visual.lm_transformer.resblocks.0.ln_2.bias', 'visual.ln_lm.weight', 'visual.ln_lm.bias', 'visual.lm_head.weight', 'visual.lm_head.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from src.clip import clip\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "model = model.to(device)\n",
    "farl_state = torch.load(\"./misc/FaRL-Base-Patch16-LAIONFace20M-ep16.pth\")\n",
    "model.load_state_dict(farl_state[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
