{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, IntEnum\n",
    "\n",
    "\n",
    "class T(IntEnum):\n",
    "    A = 0\n",
    "\n",
    "\n",
    "type(T.A)\n",
    "int(T.A)\n",
    "T.A == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "with open(\"./pickles/cdf_real2.pickle\", \"rb\") as f:\n",
    "    vlist = pickle.load(f)\n",
    "vlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "folders = [\"cropped_faces\", \"cropped_faces_250\", \"landmarks\", \"semantic_videos\", \"cropped_faces(landmark)\"]\n",
    "files = []\n",
    "for v in vlist:\n",
    "    for folder in folders:\n",
    "        # files.extend(glob(f\"/home/od/Desktop/Dataset/DFDC/{folder}/{os.path.split(v)[-1][:-4]}*.*\"))\n",
    "        files.extend(glob(v.replace(\"videos\", folder).replace(\".mp4\", \"*.*\")))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files), len(vlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "\n",
    "# video_path = \"/home/od/Desktop/Dataset/CelebDF/Fake/cropped_faces/id0_id1_0000.avi\"\n",
    "video_path = \"/home/od/Desktop/Dataset/DFDC/videos/nwpjsjarju.mp4\"\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_indices = [i for i in range(0, 300, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.load(f\"/home/od/Desktop/Dataset/DFDC/test_lm/rfwtnztcsj.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.reshape(-1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# i = random.randrange(0, len(frame_indices))\n",
    "# print(i)\n",
    "# plt.figure(figsize=(25, 15))\n",
    "# plt.imshow(cv2.cvtColor(frames[frame_indices[i]], cv2.COLOR_BGR2RGB))\n",
    "# plt.scatter(landmarks[i, :, 0], landmarks[i, :, 1], alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "\n",
    "# video_path = \"/home/od/Desktop/Dataset/CelebDF/Fake/cropped_faces/id0_id1_0000.avi\"\n",
    "video_path = \"/stock/FaceForensicC23/videos/NT/217_117.mp4\"\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceData:\n",
    "    def __init__(self, _lm, _bbox, _idx):\n",
    "        ema_lm = _lm\n",
    "        lm = [_lm]\n",
    "        bbox = [_bbox]\n",
    "        idx = [_idx]\n",
    "\n",
    "    def last(self):\n",
    "        return ema_lm\n",
    "\n",
    "    def add(self, _lm, _bbox, _idx):\n",
    "        ema_lm = ema_lm * 0.5 + _lm * 0.5\n",
    "        lm.append(_lm)\n",
    "        bbox.append(_bbox)\n",
    "        idx.append(_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./pickles/217_117.pickle\", \"rb\") as f:\n",
    "    frame_faces = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dbs = []\n",
    "for faces, index in zip(frame_faces, frame_indices):\n",
    "    frame_landmarks = np.stack([face[\"landmarks\"] for face in faces])\n",
    "    matched_indices = []\n",
    "\n",
    "    for face_data in face_dbs:\n",
    "\n",
    "        lm_diff = np.sum(\n",
    "            np.linalg.norm(frame_landmarks - face_data.last(), axis=-1),\n",
    "            axis=1\n",
    "        ) / frame_landmarks.shape[1]\n",
    "\n",
    "        if (np.min(lm_diff) > 100):\n",
    "            continue\n",
    "\n",
    "        closest_idx = np.argmin(lm_diff)\n",
    "        matched_indices.append(closest_idx)\n",
    "        face_data.add(faces[closest_idx][\"landmarks\"], faces[closest_idx][\"bbox\"], index)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        if i in matched_indices:\n",
    "            continue\n",
    "        else:\n",
    "            _landmark = face['landmarks']\n",
    "            _bbox = face['bbox']\n",
    "            face_dbs.append(FaceData(_landmark, _bbox, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(face_dbs), len(face_dbs[0].lm), len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted(face_dbs, key=lambda x: len(x), reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(0, len(frames)) if not i in face_dbs[0].idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "fid = 0\n",
    "i = random.randrange(0, len(face_dbs[fid]))\n",
    "i = 210\n",
    "landmarks = face_dbs[fid].lm[i]\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.imshow(cv2.cvtColor(frames[face_dbs[fid].idx[i]], cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], alpha=0.5, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = 211\n",
    "faces = [face[\"landmarks\"] for face in frame_faces[idx]]\n",
    "print(len(faces))\n",
    "landmarks = np.stack(faces).reshape((-1, 2))\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.imshow(cv2.cvtColor(frames[idx], cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], alpha=0.5, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FaRL weights\n",
    "import torch\n",
    "from src.clip import clip\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "model = model.to(device)\n",
    "farl_state = torch.load(\"./misc/FaRL-Base-Patch16-LAIONFace20M-ep16.pth\")\n",
    "model.load_state_dict(farl_state[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentation Effect Test Site\n",
    "import cv2\n",
    "import torchvision\n",
    "import albumentations as alb\n",
    "from matplotlib import pyplot as plt\n",
    "vid_reader = torchvision.io.VideoReader(\n",
    "    \"datasets/ffpp/real/c23/videos/000.avi\",\n",
    "    \"video\"\n",
    ")\n",
    "vid_reader.seek(2.5)\n",
    "image = next(vid_reader)\n",
    "del vid_reader\n",
    "\n",
    "x = alb.ReplayCompose(\n",
    "    [\n",
    "        # alb.RandomCropFromBorders(0.5,0.5,0.5,0.5,True)\n",
    "        alb.RandomResizedCrop(150, 150, scale=(0.5, 0.8), ratio=(1, 1), always_apply=True)\n",
    "        # alb.RandomCropNearBBox(always_apply=True)\n",
    "        # alb.RandomScale((-0.5,-0.1),always_apply=True),\n",
    "        # alb.Resize(150,150)\n",
    "        # alb.Resize(150,150,always_apply=1)\n",
    "        # alb.Affine([0.5,1.0],keep_ratio=True,always_apply=True),\n",
    "        # alb.Affine([1.5,2.0],keep_ratio=True,always_apply=True),\n",
    "    ],\n",
    "    p=1.\n",
    ")\n",
    "\n",
    "image = image[\"data\"].numpy().transpose((1, 2, 0))\n",
    "result = x(image=image)\n",
    "image = result[\"image\"]\n",
    "print(result[\"replay\"][\"transforms\"][0][\"params\"])\n",
    "# image = cv2.cvtColor(image.transpose((1,2,0)),cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = torch.randn((1, 10))\n",
    "x2 = torch.randn((1, 10))\n",
    "v1 = torch.randn((1, 10))\n",
    "v2 = torch.randn((1, 10))\n",
    "for i in range(5, 500, 5):\n",
    "    print(\"Ratio:\", i)\n",
    "    _x1 = v1 / i\n",
    "    _x2 = v2 / i\n",
    "    print(\n",
    "        torch.mean(torch.nn.functional.cosine_similarity(\n",
    "            torch.stack((x1 + _x1, x2 + _x2)), torch.stack((x1, x2)), dim=-1)),\n",
    "        torch.nn.functional.cosine_similarity(x1 + _x1, x2 + _x2, dim=-1),\n",
    "        torch.nn.functional.cosine_similarity(x1, x2, dim=-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "with open(\"misc/ow_semantic_patches.pickle\", \"rb\") as f:\n",
    "    ow_s = pickle.load(f)\n",
    "with open(\"misc/semantic_patches.pickle\", \"rb\") as f:\n",
    "    s = pickle.load(f)\n",
    "\n",
    "parts = list(s[\"k\"].keys())\n",
    "for l in range(12):\n",
    "    print(f\"L:{l}, Parts:{parts}\")\n",
    "    print(\n",
    "        torch.nn.functional.cosine_similarity(\n",
    "            torch.stack([s[\"k\"][p][l] for p in parts], dim=0).unsqueeze(0),\n",
    "            torch.stack([ow_s[\"k\"][p][l] for p in parts], dim=0).unsqueeze(1),\n",
    "            dim=-1\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for name, path in [\n",
    "    (\"10\", \"logs/test/vocal-bee-1132/best_weights.pt\"),\n",
    "    (\"20\", \"logs/test/peachy-haze-1123/best_weights.pt\"),\n",
    "    (\"50\", \"logs/test/olive-water-1118/best_weights.pt\")\n",
    "]:\n",
    "    print(f\"===={name}====\")\n",
    "    prompts = [v for k, v in torch.load(path, \"cpu\").items() if \"prompt\" in k][0]\n",
    "    result = torch.nn.functional.cosine_similarity(prompts.unsqueeze(1), prompts.unsqueeze(2), dim=-1)\n",
    "    print(result.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "width = 768\n",
    "patch_size = 14\n",
    "scale = width**-0.5\n",
    "val = math.sqrt(6. / (3 * patch_size**2 + 1) + width)\n",
    "print(val)\n",
    "layers = 12\n",
    "prompts = 10\n",
    "prompt_embeddings = nn.Parameter(scale * torch.randn(layers, prompts, width))\n",
    "# nn.init.uniform_(prompt_embeddings.data, -val, val)  # xavier_uniform initialization\n",
    "\n",
    "torch.nn.functional.cosine_similarity(prompt_embeddings.unsqueeze(\n",
    "    1), prompt_embeddings.unsqueeze(2), dim=-1).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.clip import clip\n",
    "origin = clip.load(\"ViT-B/16\", prompts=0)[0].visual.float().cpu().state_dict()\n",
    "\n",
    "new = {\n",
    "    k[8:]: v for k, v in torch.load(\"logs/test/iconic-durian-1140/best_weights.pt\", map_location=\"cpu\").items() if k[:7] == \"encoder\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
