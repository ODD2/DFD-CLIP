{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, IntEnum\n",
    "\n",
    "\n",
    "class T(IntEnum):\n",
    "    A = 0\n",
    "\n",
    "\n",
    "type(T.A)\n",
    "int(T.A)\n",
    "T.A == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "with open(\"./pickles/cdf_real2.pickle\", \"rb\") as f:\n",
    "    vlist = pickle.load(f)\n",
    "vlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "folders = [\"cropped_faces\", \"cropped_faces_250\", \"landmarks\", \"semantic_videos\", \"cropped_faces(landmark)\"]\n",
    "files = []\n",
    "for v in vlist:\n",
    "    for folder in folders:\n",
    "        # files.extend(glob(f\"/home/od/Desktop/Dataset/DFDC/{folder}/{os.path.split(v)[-1][:-4]}*.*\"))\n",
    "        files.extend(glob(v.replace(\"videos\", folder).replace(\".mp4\", \"*.*\")))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files), len(vlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "\n",
    "# video_path = \"/home/od/Desktop/Dataset/CelebDF/Fake/cropped_faces/id0_id1_0000.avi\"\n",
    "video_path = \"/home/od/Desktop/Dataset/DFDC/videos/nwpjsjarju.mp4\"\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_indices = [i for i in range(0, 300, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.load(f\"/home/od/Desktop/Dataset/DFDC/test_lm/rfwtnztcsj.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.reshape(-1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# i = random.randrange(0, len(frame_indices))\n",
    "# print(i)\n",
    "# plt.figure(figsize=(25, 15))\n",
    "# plt.imshow(cv2.cvtColor(frames[frame_indices[i]], cv2.COLOR_BGR2RGB))\n",
    "# plt.scatter(landmarks[i, :, 0], landmarks[i, :, 1], alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "\n",
    "# video_path = \"/home/od/Desktop/Dataset/CelebDF/Fake/cropped_faces/id0_id1_0000.avi\"\n",
    "video_path = \"/stock/FaceForensicC23/videos/NT/217_117.mp4\"\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceData:\n",
    "    def __init__(self, _lm, _bbox, _idx):\n",
    "        ema_lm = _lm\n",
    "        lm = [_lm]\n",
    "        bbox = [_bbox]\n",
    "        idx = [_idx]\n",
    "\n",
    "    def last(self):\n",
    "        return ema_lm\n",
    "\n",
    "    def add(self, _lm, _bbox, _idx):\n",
    "        ema_lm = ema_lm * 0.5 + _lm * 0.5\n",
    "        lm.append(_lm)\n",
    "        bbox.append(_bbox)\n",
    "        idx.append(_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./pickles/217_117.pickle\", \"rb\") as f:\n",
    "    frame_faces = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dbs = []\n",
    "for faces, index in zip(frame_faces, frame_indices):\n",
    "    frame_landmarks = np.stack([face[\"landmarks\"] for face in faces])\n",
    "    matched_indices = []\n",
    "\n",
    "    for face_data in face_dbs:\n",
    "\n",
    "        lm_diff = np.sum(\n",
    "            np.linalg.norm(frame_landmarks - face_data.last(), axis=-1),\n",
    "            axis=1\n",
    "        ) / frame_landmarks.shape[1]\n",
    "\n",
    "        if (np.min(lm_diff) > 100):\n",
    "            continue\n",
    "\n",
    "        closest_idx = np.argmin(lm_diff)\n",
    "        matched_indices.append(closest_idx)\n",
    "        face_data.add(faces[closest_idx][\"landmarks\"], faces[closest_idx][\"bbox\"], index)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        if i in matched_indices:\n",
    "            continue\n",
    "        else:\n",
    "            _landmark = face['landmarks']\n",
    "            _bbox = face['bbox']\n",
    "            face_dbs.append(FaceData(_landmark, _bbox, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(face_dbs), len(face_dbs[0].lm), len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted(face_dbs, key=lambda x: len(x), reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(0, len(frames)) if not i in face_dbs[0].idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "fid = 0\n",
    "i = random.randrange(0, len(face_dbs[fid]))\n",
    "i = 210\n",
    "landmarks = face_dbs[fid].lm[i]\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.imshow(cv2.cvtColor(frames[face_dbs[fid].idx[i]], cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], alpha=0.5, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = 211\n",
    "faces = [face[\"landmarks\"] for face in frame_faces[idx]]\n",
    "print(len(faces))\n",
    "landmarks = np.stack(faces).reshape((-1, 2))\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.imshow(cv2.cvtColor(frames[idx], cv2.COLOR_BGR2RGB))\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], alpha=0.5, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FaRL weights\n",
    "import torch\n",
    "from src.clip import clip\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "model = model.to(device)\n",
    "farl_state = torch.load(\"./misc/FaRL-Base-Patch16-LAIONFace20M-ep16.pth\")\n",
    "model.load_state_dict(farl_state[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentation Effect Test Site\n",
    "import cv2\n",
    "import torchvision\n",
    "import albumentations as alb\n",
    "from matplotlib import pyplot as plt\n",
    "vid_reader = torchvision.io.VideoReader(\n",
    "    \"datasets/ffpp/real/c23/videos/000.avi\",\n",
    "    \"video\"\n",
    ")\n",
    "vid_reader.seek(2.5)\n",
    "image = next(vid_reader)\n",
    "del vid_reader\n",
    "\n",
    "x = alb.ReplayCompose(\n",
    "    [\n",
    "        # alb.RandomCropFromBorders(0.5,0.5,0.5,0.5,True)\n",
    "        alb.RandomResizedCrop(150, 150, scale=(0.5, 0.8), ratio=(1, 1), always_apply=True)\n",
    "        # alb.RandomCropNearBBox(always_apply=True)\n",
    "        # alb.RandomScale((-0.5,-0.1),always_apply=True),\n",
    "        # alb.Resize(150,150)\n",
    "        # alb.Resize(150,150,always_apply=1)\n",
    "        # alb.Affine([0.5,1.0],keep_ratio=True,always_apply=True),\n",
    "        # alb.Affine([1.5,2.0],keep_ratio=True,always_apply=True),\n",
    "    ],\n",
    "    p=1.\n",
    ")\n",
    "\n",
    "image = image[\"data\"].numpy().transpose((1, 2, 0))\n",
    "result = x(image=image)\n",
    "image = result[\"image\"]\n",
    "print(result[\"replay\"][\"transforms\"][0][\"params\"])\n",
    "# image = cv2.cvtColor(image.transpose((1,2,0)),cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = torch.randn((1, 10))\n",
    "x2 = torch.randn((1, 10))\n",
    "v1 = torch.randn((1, 10))\n",
    "v2 = torch.randn((1, 10))\n",
    "for i in range(5, 500, 5):\n",
    "    print(\"Ratio:\", i)\n",
    "    _x1 = v1 / i\n",
    "    _x2 = v2 / i\n",
    "    print(\n",
    "        torch.mean(torch.nn.functional.cosine_similarity(\n",
    "            torch.stack((x1 + _x1, x2 + _x2)), torch.stack((x1, x2)), dim=-1)),\n",
    "        torch.nn.functional.cosine_similarity(x1 + _x1, x2 + _x2, dim=-1),\n",
    "        torch.nn.functional.cosine_similarity(x1, x2, dim=-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "with open(\"misc/ow_semantic_patches.pickle\", \"rb\") as f:\n",
    "    ow_s = pickle.load(f)\n",
    "with open(\"misc/semantic_patches.pickle\", \"rb\") as f:\n",
    "    s = pickle.load(f)\n",
    "\n",
    "parts = list(s[\"k\"].keys())\n",
    "for l in range(12):\n",
    "    print(f\"L:{l}, Parts:{parts}\")\n",
    "    print(\n",
    "        torch.nn.functional.cosine_similarity(\n",
    "            torch.stack([s[\"k\"][p][l] for p in parts], dim=0).unsqueeze(0),\n",
    "            torch.stack([ow_s[\"k\"][p][l] for p in parts], dim=0).unsqueeze(1),\n",
    "            dim=-1\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for name, path in [\n",
    "    (\"10\", \"logs/test/vocal-bee-1132/best_weights.pt\"),\n",
    "    (\"20\", \"logs/test/peachy-haze-1123/best_weights.pt\"),\n",
    "    (\"50\", \"logs/test/olive-water-1118/best_weights.pt\")\n",
    "]:\n",
    "    print(f\"===={name}====\")\n",
    "    prompts = [v for k, v in torch.load(path, \"cpu\").items() if \"prompt\" in k][0]\n",
    "    result = torch.nn.functional.cosine_similarity(prompts.unsqueeze(1), prompts.unsqueeze(2), dim=-1)\n",
    "    print(result.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "width = 768\n",
    "patch_size = 14\n",
    "scale = width**-0.5\n",
    "val = math.sqrt(6. / (3 * patch_size**2 + 1) + width)\n",
    "print(val)\n",
    "layers = 12\n",
    "prompts = 10\n",
    "prompt_embeddings = nn.Parameter(scale * torch.randn(layers, prompts, width))\n",
    "# nn.init.uniform_(prompt_embeddings.data, -val, val)  # xavier_uniform initialization\n",
    "\n",
    "torch.nn.functional.cosine_similarity(prompt_embeddings.unsqueeze(\n",
    "    1), prompt_embeddings.unsqueeze(2), dim=-1).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.clip import clip\n",
    "origin = clip.load(\"ViT-B/16\", prompts=0)[0].visual.float().cpu().state_dict()\n",
    "\n",
    "new = {\n",
    "    k[8:]: v for k, v in torch.load(\"logs/test/iconic-durian-1140/best_weights.pt\", map_location=\"cpu\").items() if k[:7] == \"encoder\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import torchvision\n",
    "torchvision.set_video_backend(\"video_reader\")\n",
    "num_sample = 5\n",
    "for file in glob(\"./datasets/ffpp/*/c23/videos/*\"):\n",
    "    segs = file.split('/')\n",
    "    vid_reader = torchvision.io.VideoReader(\n",
    "        file,\n",
    "        \"video\"\n",
    "    )\n",
    "    # - frames per second\n",
    "    vid_duration = vid_reader.get_metadata()[\"video\"][\"duration\"][0]\n",
    "    for cnt_frame in range(num_sample):\n",
    "        frame_path = f\"./misc/extern/samples/{segs[3]}_{segs[-1][:-4]}F{str(cnt_frame).zfill(3)}.jpg\"\n",
    "        vid_reader.seek(cnt_frame * vid_duration / num_sample)\n",
    "        frame = next(vid_reader)\n",
    "        torchvision.io.write_jpeg(frame[\"data\"], frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_reader = torchvision.io.VideoReader(\n",
    "    \"datasets/ffpp/real/c23/videos/000.avi\",\n",
    "    \"video\"\n",
    ")\n",
    "# - frames per second\n",
    "print(vid_reader.get_metadata()[\"video\"][\"duration\"][0])\n",
    "del vid_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models import VPT, remap_weight\n",
    "from src.clip import clip\n",
    "\n",
    "\n",
    "class Obj:\n",
    "    pass\n",
    "\n",
    "\n",
    "encoder = clip.load(\n",
    "    \"ViT-B/16\",\n",
    "    frame_prompts=20,\n",
    "    video_prompts=0\n",
    ")[0].visual.float()\n",
    "\n",
    "\n",
    "# w_path = \"logs/test/peachy-haze-1123/best_weights.pt\"  # best no qg\n",
    "w_path = \"logs/test/0804T0901/best_weights.pt\"  # with qg\n",
    "encoder.load_state_dict(\n",
    "    remap_weight(\n",
    "        {\n",
    "            **{\n",
    "                k[8:]: v for k, v in torch.load(w_path, map_location=\"cpu\").items() if k[:7] == \"encoder\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    ")\n",
    "encoder.eval()\n",
    "encoder.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./misc/real_semantic_patches_v2_1000.pickle\", \"rb\") as f:\n",
    "    prefetch_queries = pickle.load(f)\n",
    "    semantic_queries = []\n",
    "    for i in range(12):\n",
    "        layer_results = []\n",
    "        for part in [\"lips\", \"skin\", \"eyes\", \"nose\", \"eyebrows\"]:\n",
    "            layer_results.append(prefetch_queries[\"k\"][part][i])\n",
    "        semantic_queries.append(torch.stack(layer_results))\n",
    "    semantic_queries = torch.stack(semantic_queries)\n",
    "    semantic_queries.requires_grad_(False)\n",
    "    semantic_queries = semantic_queries.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize accelerator and trackers (if enabled)\n",
    "from os import makedirs, path, scandir\n",
    "import pickle\n",
    "import cv2\n",
    "import json\n",
    "from yacs.config import CfgNode as CN\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "# from src.datasets import FFPP,RPPG\n",
    "from accelerate import Accelerator\n",
    "from main import get_config, init_accelerator, set_seed, FFPP, DFDC, CDF\n",
    "logging.basicConfig(level=\"DEBUG\", format='[%(levelname)s][%(filename)s:%(lineno)d]: %(message)s')\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(224, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(224),\n",
    "    T.ConvertImageDtype(torch.float32),\n",
    "    T.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "c = FFPP.get_default_config()\n",
    "c.augmentation = \"normal+frame\"\n",
    "c.contrast = 1\n",
    "c.compressions = [\"c23\"]\n",
    "c.types = [\"REAL\", \"DF\", \"FS\", \"F2F\", \"NT\"]\n",
    "c.root_dir = \"./datasets/ffpp/\"\n",
    "c.vid_ext = \".avi\"\n",
    "accelerator = Accelerator(mixed_precision='no')\n",
    "x = FFPP(c, 1, 5, transform, accelerator, split=\"test\")\n",
    "frames, label, mask, _, _ = x[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    # get key and value from each CLIP ViT layer\n",
    "    kvs = encoder(frames[0][\"c23\"].to(\"cuda\"), with_q=True, with_out=True, with_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for i, kv in enumerate(kvs):\n",
    "    aff = torch.einsum('nqhc,nkhc->nqkh', kv[\"q\"] / (kv[\"q\"].size(-1) ** 0.5), kv[\"k\"])\n",
    "    aff = aff.softmax(dim=-2)\n",
    "\n",
    "    print(f\"Layer:{i}\")\n",
    "    cls_prompt_score = aff[:, 0, 1:21]\n",
    "    patch_prompt_score = aff[:, 21:, 1:21]\n",
    "    prompt_prompt_score = aff[:, 1:21:, 1:21]\n",
    "    print(\n",
    "        \"CLS:\\n\",\n",
    "        f\"mean'\\t={[round(v,3) for v in (cls_prompt_score.mean(dim=1).tolist()[0])]}\\n\",\n",
    "        f\"max\\t={[round(v,3) for v in torch.max(aff[:,0],dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"max'\\t={[round(v,3) for v in torch.max(cls_prompt_score,dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"min\\t={[round(v,3) for v in torch.min(aff[:,0],dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"min'\\t={[round(v,3) for v in torch.min(cls_prompt_score,dim=1)[0].tolist()[0]]}\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"PATCH:\\n\",\n",
    "        f\"mean'\\t={[round(v,3) for v in (patch_prompt_score.mean(dim=1).mean(dim=1).tolist()[0])]}\\n\",\n",
    "        f\"max\\t={[round(v,3) for v in torch.max(aff[:,21:].mean(1),dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"max'\\t={[round(v,3) for v in torch.max(patch_prompt_score.mean(dim=1),dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"min\\t={[round(v,3) for v in torch.min(aff[:,21:].mean(1),dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"min'\\t={[round(v,3) for v in torch.min(patch_prompt_score.mean(dim=1),dim=1)[0].tolist()[0]]}\"\n",
    "    )\n",
    "    cos_sim = torch.max(torch.nn.functional.cosine_similarity(\n",
    "        kv[\"q\"][:, 1:21].flatten(-2).unsqueeze(2),\n",
    "        semantic_queries[i].unsqueeze(0).unsqueeze(0),\n",
    "        dim=-1\n",
    "    ), dim=-1)[0].mean(1).tolist()[0]\n",
    "    # print(cos_sim.shape)\n",
    "    # break\n",
    "    print(\n",
    "        \"PROMPT:\\n\",\n",
    "        f\"mean'\\t={[round(v,3) for v in (prompt_prompt_score.mean(dim=1).mean(dim=1).tolist()[0])]}\\n\",\n",
    "        f\"max\\t={[round(v,3) for v in torch.max(aff[:,1:21].mean(1),dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"max'\\t={[round(v,3) for v in torch.max(prompt_prompt_score.mean(dim=1),dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"min\\t={[round(v,3) for v in torch.min(aff[:,1:21].mean(1),dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"min'\\t={[round(v,3) for v in torch.min(prompt_prompt_score.mean(dim=1),dim=1)[0].tolist()[0]]}\\n\",\n",
    "        f\"cssm\\t={round(cos_sim,3)}\"\n",
    "    )\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
