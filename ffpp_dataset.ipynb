{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import path, scandir\n",
    "# videos = {f.name: [] for f in scandir(path.join(\"./datasets/ffpp/real/\", f'raw/videos')) if f.is_dir()}\n",
    "# videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize accelerator and trackers (if enabled)\n",
    "from os import makedirs,path,scandir\n",
    "import pickle\n",
    "import cv2\n",
    "import json\n",
    "from yacs.config import CfgNode as CN\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "# from src.datasets import FFPP,RPPG\n",
    "from main import get_config,init_accelerator,set_seed,RPPG\n",
    "\n",
    "\n",
    "\n",
    "class FFPP(Dataset):\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        C.name = 'train'\n",
    "        C.root_dir = './datasets/ffpp/'\n",
    "        C.detection_level = 'video'\n",
    "        C.train_ratio = 0.95\n",
    "        C.types = ['REAL', 'DF']\n",
    "        C.compressions = ['raw']\n",
    "        C.dataset = \"FFPP\"\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config,num_frames,clip_duration, transform=None, accelerator=None, split='train'):\n",
    "        self.TYPE_DIRS = {\n",
    "            'REAL': 'real/',\n",
    "            # 'DFD' : 'data/original_sequences/actors/',\n",
    "            'DF'  : 'DF/',\n",
    "            'FS'  : 'FS/',\n",
    "            'F2F' : 'F2F/',\n",
    "            'NT'  : 'NT/',\n",
    "            # 'FSH' : 'data/manipulated_sequences/FaceShifter/',\n",
    "            # 'DFD-FAKE' : 'data/manipulated_sequences/DeepFakeDetection/',\n",
    "        }\n",
    "        self.name = config.name\n",
    "        self.root = path.expanduser(config.root_dir)\n",
    "        self.detection_level = config.detection_level\n",
    "        self.types = config.types\n",
    "        self.compressions = config.compressions\n",
    "        self.num_frames = num_frames\n",
    "        self.clip_duration = clip_duration\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        # available clips per data\n",
    "        self.video_list = []\n",
    "\n",
    "        # stacking data clips\n",
    "        self.stack_video_clips = []\n",
    "\n",
    "        self._build_video_table(accelerator)\n",
    "        self._build_video_list(accelerator)\n",
    "        \n",
    "    def _build_video_table(self, accelerator):\n",
    "        self.video_table = {}\n",
    "\n",
    "        progress_bar = tqdm(self.types, disable=not accelerator.is_local_main_process)\n",
    "        for df_type in progress_bar:\n",
    "            self.video_table[df_type] = {}\n",
    "            for comp in self.compressions:\n",
    "                video_cache = path.expanduser(f'./.cache/dfd-clip/videos/{df_type}-{comp}.pkl')\n",
    "                if path.isfile(video_cache):\n",
    "                    with open(video_cache, 'rb') as f:\n",
    "                        videos = pickle.load(f)\n",
    "                    self.video_table[df_type][comp] = videos\n",
    "                    continue\n",
    "\n",
    "                # subdir\n",
    "                subdir = path.join(self.root, self.TYPE_DIRS[df_type], f'{comp}/videos')\n",
    "\n",
    "                video_metas =  {}\n",
    "\n",
    "                # video table\n",
    "                for f in  scandir(subdir):\n",
    "                    if '.avi' in f.name:\n",
    "                        cap = cv2.VideoCapture(f.path)\n",
    "                        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "                        frames = round(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                        video_metas[f.name[:-4]] ={\n",
    "                            \"fps\" : fps,\n",
    "                            \"frames\" : frames,\n",
    "                            \"duration\": frames/fps,\n",
    "                            \"path\": f.path\n",
    "                        }\n",
    "                        cap.release()\n",
    "                \n",
    "                # description\n",
    "                progress_bar.set_description(f\"{df_type}: {comp}/videos\")\n",
    "\n",
    "                # caching\n",
    "                if accelerator.is_local_main_process:\n",
    "                    makedirs(path.dirname(video_cache), exist_ok=True)\n",
    "                    with open(video_cache, 'wb') as f:\n",
    "                        pickle.dump(video_metas, f)\n",
    "\n",
    "                self.video_table[df_type][comp] = video_metas\n",
    "        \n",
    "    def _build_video_list(self, accelerator):\n",
    "        self.video_list = []\n",
    "        \n",
    "        with open(path.join(self.root, 'splits', f'{self.split}.json')) as f:\n",
    "            idxs = json.load(f)\n",
    "            \n",
    "        for df_type in self.types:\n",
    "            for comp in self.compressions:\n",
    "                adj_idxs = [i for inner in idxs for i in inner] if df_type == 'REAL' else ['_'.join(idx) for idx in idxs] + ['_'.join(reversed(idx)) for idx in idxs]\n",
    "\n",
    "                for idx in adj_idxs:\n",
    "                    if idx in self.video_table[df_type][comp]:\n",
    "                        clips = int(self.video_table[df_type][comp][idx][\"duration\"]//self.clip_duration)\n",
    "                        self.video_list.append((df_type, comp, idx, clips))\n",
    "                    else:\n",
    "                        accelerator.print(f'Warning: video {path.join(self.root, self.TYPE_DIRS[df_type], comp, \"videos\", idx)} does not present in the processed dataset.')\n",
    "\n",
    "        # stacking up the amount of data clips for further usage\n",
    "        self.stack_video_clips = [0]\n",
    "        for _,_,_,i in self.video_list:\n",
    "            self.stack_video_clips.append(self.stack_video_clips[-1] + i)\n",
    "        self.stack_video_clips.pop(0)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.stack_video_clips[-1]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        result = self.get_dict(idx)\n",
    "        return result[\"frames\"],result[\"label\"],result[\"mask\"]\n",
    "\n",
    "\n",
    "    def get_dict(self,idx):\n",
    "        while(True):\n",
    "            try:\n",
    "                video_idx =  next(i for i,x in enumerate(self.stack_video_clips) if  idx < x)\n",
    "                df_type, comp, video_name, clips = self.video_list[video_idx]\n",
    "                video_meta = self.video_table[df_type][comp][video_name]\n",
    "                video_offset_duration =  (idx - (0 if video_idx == 0 else self.stack_video_clips[video_idx-1]))*self.clip_duration\n",
    "\n",
    "                # video frame processing\n",
    "                frames = []\n",
    "                cap = cv2.VideoCapture(video_meta[\"path\"])\n",
    "                # - frames per second\n",
    "                video_sample_freq = video_meta[\"fps\"]\n",
    "                # - the amount of frames to skip\n",
    "                video_sample_offset = int(video_sample_freq * video_offset_duration)\n",
    "                # - the amount of frames for the duration of a clip\n",
    "                video_clip_samples = int(video_sample_freq * self.clip_duration)\n",
    "                # - the amount of frames to skip in order to meet the num_frames per clip.(excluding the head & tail frames )\n",
    "                video_sample_stride = (video_clip_samples-1) / (self.num_frames - 1)\n",
    "                # - fast forward to the the sampling start.\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES,video_sample_offset)\n",
    "                # - fetch frames of clip duration\n",
    "                next_sample_idx = 0\n",
    "                for sample_idx in range(video_clip_samples):\n",
    "                    ret, frame = cap.read()\n",
    "                    if(ret):\n",
    "                        if(sample_idx == next_sample_idx):\n",
    "                            frames.append(torch.from_numpy(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB).transpose((2,0,1))))\n",
    "                            next_sample_idx = int(round(len(frames) * video_sample_stride))\n",
    "                    else:\n",
    "                        raise NotImplementedError()\n",
    "                frames = torch.stack(frames)\n",
    "\n",
    "\n",
    "                # transformation\n",
    "                if (self.transform):\n",
    "                    frames = self.transform(frames)\n",
    "\n",
    "                # padding and masking missing frames.\n",
    "                mask = torch.tensor([1.] * len(frames) +\n",
    "                                    [0.] * (self.num_frames - len(frames)), dtype=torch.bool)\n",
    "                if len(frames) < self.num_frames:\n",
    "                    diff = self.num_frames - len(frames)\n",
    "                    padding = torch.zeros((diff, *frames.shape[1:]),dtype=torch.uint8)\n",
    "                    frames = torch.concatenate((frames, padding))\n",
    "                \n",
    "                return {\n",
    "                    \"frames\":frames,\n",
    "                    \"label\": 0 if df_type == \"REAL\" else 1,\n",
    "                    \"mask\":mask,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error occur: {e}\")\n",
    "                idx = random.randrange(0,len(self))\n",
    "\n",
    "c = get_config(\"./configs/mix.yml\")\n",
    "\n",
    "# x = FFPP(c.data.train[0],c.data.num_frames,c.data.clip_duration,lambda x: x,accelerator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'architecture': 'ViT-B/16', 'decode_stride': 2, 'dropout': 0.0, 'out_dim': [180, 2], 'losses': [['log_softmax', 'kl_div'], ['auc_roc']]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.trainer.metrics\n",
    "'options' in c.trainer.metrics[1].types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typ,cmp,idx,_ = x.video_list[5]\n",
    "# x.video_table[typ][cmp][idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames,label,mask = x[7]\n",
    "# (len(frames),label,len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(30,150))\n",
    "# plt.imshow(np.stack(frames[:30].numpy().transpose((0,2,3,1)),axis=1).reshape((150,-1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2059/2059 [05:21<00:00,  6.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(x))):\n",
    "    try:\n",
    "        x[i]\n",
    "    except Exception as e:\n",
    "        print(f\"Error Occur at {i}:{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9d400f0712968c6b0c2c185442708fcff6ca365f2120d94e1d89a9df5bd30ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
