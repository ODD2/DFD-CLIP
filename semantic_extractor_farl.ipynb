{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import facer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from preprocessing.extract_faces import get_video_clip, save_video_lossless\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "video_path = \"/stock/FaceForensicC23/cropped_faces/F2F/010_005.avi\"\n",
    "landmark_path = video_path.replace(\n",
    "    \"cropped_faces\", \"cropped_faces(landmark)\"\n",
    ").replace(\"avi\", \"npy\")\n",
    "\n",
    "fps, frames = get_video_clip(video_path, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.stack(frames)\n",
    "frames = frames.transpose((0, 3, 1, 2))\n",
    "frames = torch.from_numpy(frames).to(device)\n",
    "image_ids = torch.tensor([i for i in range(frames.shape[0])], device=device)\n",
    "\n",
    "landmarks = np.load(landmark_path)\n",
    "landmarks = torch.from_numpy(\n",
    "    np.stack([\n",
    "        np.stack([\n",
    "            np.mean(landmarks[f, idxs - 16], axis=0) for idxs in [\n",
    "                np.array([i for i in range(37, 43)]),\n",
    "                np.array([i for i in range(43, 49)]),\n",
    "                np.array([34]),\n",
    "                np.array([49]),\n",
    "                np.array([55])\n",
    "            ]\n",
    "        ]) for f in range(landmarks.shape[0])\n",
    "    ])\n",
    ").float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_parser = facer.face_parser(\n",
    "    'farl/lapa/448', device=device\n",
    ")  # optional \"farl/celebm/448\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.ceil((frames.shape[0] + 1) / 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "result = []\n",
    "bsize = 35\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(frames.shape[0] // bsize + 1)):\n",
    "        batch_frames = frames[i * bsize:(i + 1) * bsize]\n",
    "        batch_landmarks = landmarks[i * bsize:(i + 1) * bsize]\n",
    "        assert batch_frames.shape[0] == batch_landmarks.shape[0]\n",
    "        faces = face_parser(\n",
    "            batch_frames,\n",
    "            {\n",
    "                \"points\": batch_landmarks,\n",
    "                \"image_ids\": torch.arange(0, batch_landmarks.shape[0]).to(device)\n",
    "            }\n",
    "        )\n",
    "        result.append(\n",
    "            faces[\"seg\"][\"logits\"].cpu()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_logits = torch.cat(result, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_probs = seg_logits.softmax(dim=1)  # nfaces x nclasses x h x w\n",
    "n_classes = seg_probs.size(1)\n",
    "seg_label_img = seg_probs.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(seg_label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_seg_probs = seg_label_img.float() / n_classes * 255\n",
    "# vis_img = vis_seg_probs.sum(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(vis_seg_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum((seg_label_img[2] == vis_seg_probs[2]) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(vis_seg_probs[0].numpy().astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfd-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
